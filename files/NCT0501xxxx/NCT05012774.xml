<clinical_study>
  <!-- This xml conforms to an XML Schema at:
    https://clinicaltrials.gov/ct2/html/images/info/public.xsd -->
  <required_header>
    <download_date>ClinicalTrials.gov processed this data on November 19, 2021</download_date>
    <link_text>Link to the current ClinicalTrials.gov record.</link_text>
    <url>https://clinicaltrials.gov/show/NCT05012774</url>
  </required_header>
  <id_info>
    <org_study_id>DC015418</org_study_id>
    <nct_id>NCT05012774</nct_id>
  </id_info>
  <brief_title>Speech Perception Training: Advanced Scoring and Feedback Methods</brief_title>
  <official_title>Speech Perception Training: Advanced Scoring and Feedback Methods</official_title>
  <sponsors>
    <lead_sponsor>
      <agency>SeeHear LLC</agency>
      <agency_class>Industry</agency_class>
    </lead_sponsor>
    <collaborator>
      <agency>George Washington University</agency>
      <agency_class>Other</agency_class>
    </collaborator>
  </sponsors>
  <source>SeeHear LLC</source>
  <oversight_info>
    <has_dmc>No</has_dmc>
    <is_fda_regulated_drug>No</is_fda_regulated_drug>
    <is_fda_regulated_device>No</is_fda_regulated_device>
  </oversight_info>
  <brief_summary>
    <textblock>
      The purpose of the study is to assess training of visual speech (lipreading) and audiovisual&#xD;
      (lipreading plus auditory) speech as a rehabilitation strategy for hearing loss in adults.&#xD;
    </textblock>
  </brief_summary>
  <detailed_description>
    <textblock>
      It is hypothesized that training that improves lipreading in older adults can carry over to&#xD;
      untrained lipreading materials and to audiovisual speech recognition in noise. Participants&#xD;
      receive pre- and post-training tests of speech recognition with visual-only, auditory-only,&#xD;
      and audiovisual spoken sentences and isolated words. During pre- and post-training tests,&#xD;
      they carry out forced choice identification of lipread consonants. Participants who are&#xD;
      assigned to a training arm train on lipreading isolated sentences (Aim 1) or on learning via&#xD;
      lipreading sets of isolated nonsense words (Aim 2). Depending on the outcomes of Aims 1 and&#xD;
      2, in Aim 3, new participants receive audiovisual training with conditions from Aims 1 and 2.&#xD;
    </textblock>
  </detailed_description>
  <overall_status>Recruiting</overall_status>
  <start_date type="Actual">July 14, 2021</start_date>
  <completion_date type="Anticipated">August 31, 2022</completion_date>
  <primary_completion_date type="Anticipated">August 31, 2022</primary_completion_date>
  <phase>N/A</phase>
  <study_type>Interventional</study_type>
  <has_expanded_access>No</has_expanded_access>
  <study_design_info>
    <allocation>Randomized</allocation>
    <intervention_model>Parallel Assignment</intervention_model>
    <intervention_model_description>In Aim 1, which is sentence lipreading training, the intervention compares the provision of three different types of feedback for lipreading. The three types are consonant-level, word-level, or sentence-level feedback. A no-training wait-list control is included.&#xD;
In Aim 2, novel word training, the intervention has one arm in which novel word learning via lipreading the words is supported by allowing the participants to read the consonants in the novel words before lipreading them.&#xD;
In Aim 3, the most effective feedback with sentence training will be compared with novel word training using training stimuli that are audiovisual speech in speech-shaped noise.&#xD;
The interventions are all delivered individually via the web. Finally, all pre- and post-training results will be compared across groups including the wait-list controls.</intervention_model_description>
    <primary_purpose>Treatment</primary_purpose>
    <masking>None (Open Label)</masking>
  </study_design_info>
  <primary_outcome>
    <measure>Change in words-correct scores across pre- and post-training sentence lipreading tests.</measure>
    <time_frame>up to 8 weeks</time_frame>
    <description>Change in words-correct scores across pre- and post-training visual-only sentence tests with open-set responses.</description>
  </primary_outcome>
  <primary_outcome>
    <measure>Change in phonemes-correct scores across pre- and post-training sentence lipreading tests.</measure>
    <time_frame>up to 8 weeks</time_frame>
    <description>Change in phonemes-correct scores across pre- and post-training visual-only sentence tests with open-set responses.</description>
  </primary_outcome>
  <primary_outcome>
    <measure>Change in consonant-identification scores across pre- and post-training word lipreading tests.</measure>
    <time_frame>up to 8 weeks</time_frame>
    <description>Change in consonants-correct scores across pre- and post-training for visual-only tests using isolated nonsense words and closed-set responses.</description>
  </primary_outcome>
  <primary_outcome>
    <measure>Change in word-correct scores across pre- and post-training word lipreading tests.</measure>
    <time_frame>up to 8 weeks</time_frame>
    <description>Change in words-correct scores across pre- and post-training visual-only isolated real word tests with open-set response.</description>
  </primary_outcome>
  <primary_outcome>
    <measure>Change in phonemes-correct scores across pre- and post-training word lipreading tests.</measure>
    <time_frame>up to 8 weeks</time_frame>
    <description>Change in phonemes-correct scores across pre- and post-training visual-only isolated real word tests with open-set response.</description>
  </primary_outcome>
  <secondary_outcome>
    <measure>Change in words-correct scores across pre- and post-training audiovisual sentence tests.</measure>
    <time_frame>up to 8 weeks</time_frame>
    <description>Change in words-correct scores across pre- and post-training audiovisual-in-noise sentence identification tests with open-set response.</description>
  </secondary_outcome>
  <secondary_outcome>
    <measure>Change in phonemes-correct scores across pre- and post-training audiovisual sentence tests.</measure>
    <time_frame>up to 8 weeks</time_frame>
    <description>Change in phonemes-correct scores across pre- and post-training audiovisual-in-noise sentence identification tests with open-set response.</description>
  </secondary_outcome>
  <secondary_outcome>
    <measure>Change in word-correct scores across pre- and post-training for audiovisual-in-noise word tests.</measure>
    <time_frame>up to 8 weeks</time_frame>
    <description>Change in word-correct scores across pre- and post-training audiovisual-in-noise isolated real word tests with open-set response.</description>
  </secondary_outcome>
  <secondary_outcome>
    <measure>Change in phoneme-recognition scores across pre- and post-training audiovisual-in-noise word tests.</measure>
    <time_frame>up to 8 weeks</time_frame>
    <description>Change in phoneme-recognition scores across pre- and post-training audiovisual-in-noise isolated real word tests with open-set response.</description>
  </secondary_outcome>
  <number_of_arms>7</number_of_arms>
  <enrollment type="Anticipated">126</enrollment>
  <condition>Hearing Loss</condition>
  <arm_group>
    <arm_group_label>Aim 1, Sentence Training: Sentence Feedback</arm_group_label>
    <arm_group_type>Experimental</arm_group_type>
    <description>In Aim 1, which is sentence lipreading training, the intervention compares the provision of three different types of feedback for lipreading.&#xD;
This arm gives printed whole sentence feedback following an attempt to lipread each sentence.&#xD;
Participants receive pre- and post-training tests.</description>
  </arm_group>
  <arm_group>
    <arm_group_label>Aim 1, Sentence Training: Word Feedback</arm_group_label>
    <arm_group_type>Experimental</arm_group_type>
    <description>In Aim 1, which is sentence lipreading training, the intervention compares the provision of three different types of feedback for lipreading.&#xD;
This arm gives printed whole word feedback following an attempt to lipread each sentence. Word feedback is for correct words and words that are perceptually similar but incorrect responses.&#xD;
Participants receive pre- and post-training tests.</description>
  </arm_group>
  <arm_group>
    <arm_group_label>Aim 1, Sentence Training: Consonant Feedback</arm_group_label>
    <arm_group_type>Experimental</arm_group_type>
    <description>In Aim 1, which is sentence lipreading training, the intervention compares the provision of three different types of feedback for lipreading.&#xD;
This arm gives printed consonant word feedback following an attempt to lipread each sentence. Word feedback is for correct words, but only the consonants are given as feedback for words that are perceptually similar but incorrect responses.&#xD;
Participants receive pre- and post-training tests.</description>
  </arm_group>
  <arm_group>
    <arm_group_label>Aim 1, Sentence Training: No Training Control</arm_group_label>
    <arm_group_type>No Intervention</arm_group_type>
    <description>Participants receive only the pre- and post-training tests.</description>
  </arm_group>
  <arm_group>
    <arm_group_label>Aim 2, Nonsense Word Training</arm_group_label>
    <arm_group_type>Experimental</arm_group_type>
    <description>Participants train to lipread nonsense words that name nonsense pictures. Participants receive pre- and post-training tests.</description>
  </arm_group>
  <arm_group>
    <arm_group_label>Aim 3, Audiovisual Nonsense Word Training</arm_group_label>
    <arm_group_type>Experimental</arm_group_type>
    <description>Participants train to recognize audiovisual spoken nonsense words that name nonsense pictures and are presented in speech-shaped noise. The paradigm is the same as in Aim 2.&#xD;
Participants receive pre- and post-training tests.</description>
  </arm_group>
  <arm_group>
    <arm_group_label>Aim 3, Audiovisual Sentence Training</arm_group_label>
    <arm_group_type>Experimental</arm_group_type>
    <description>Participants receive the same training paradigm from Aim 1 with the most effective feedback type from Aim 1. But the sentences are audiovisual and in speech-shaped noise.&#xD;
Participants receive pre- and post-training tests.</description>
  </arm_group>
  <intervention>
    <intervention_type>Behavioral</intervention_type>
    <intervention_name>Speech Recognition Training</intervention_name>
    <description>Training for each training arm takes place over 10 sessions carried out at home. on the participant's own computer without experimenter direct supervision. Pre- and post-training tests are carried out under experimenter supervision.</description>
    <arm_group_label>Aim 1, Sentence Training: Consonant Feedback</arm_group_label>
    <arm_group_label>Aim 1, Sentence Training: Sentence Feedback</arm_group_label>
    <arm_group_label>Aim 1, Sentence Training: Word Feedback</arm_group_label>
    <arm_group_label>Aim 2, Nonsense Word Training</arm_group_label>
    <arm_group_label>Aim 3, Audiovisual Nonsense Word Training</arm_group_label>
    <arm_group_label>Aim 3, Audiovisual Sentence Training</arm_group_label>
  </intervention>
  <eligibility>
    <criteria>
      <textblock>
        Inclusion Criteria:&#xD;
&#xD;
          -  Must have Mild to severe hearing loss&#xD;
&#xD;
          -  Native speaker of American English&#xD;
&#xD;
          -  Access to computer and the ability to use it for the testing, training, and&#xD;
             teleconferencing&#xD;
&#xD;
          -  computer display &gt;= 10 inches diagonal&#xD;
&#xD;
        Exclusion Criteria:&#xD;
&#xD;
          -  Internet too slow for streaming&#xD;
&#xD;
          -  History of brain trauma or learning disability&#xD;
&#xD;
          -  Poor (corrected) vision&#xD;
      </textblock>
    </criteria>
    <gender>All</gender>
    <minimum_age>55 Years</minimum_age>
    <maximum_age>80 Years</maximum_age>
    <healthy_volunteers>Accepts Healthy Volunteers</healthy_volunteers>
  </eligibility>
  <overall_official>
    <last_name>Lynne E Bernstein, Ph.D.</last_name>
    <role>Study Director</role>
    <affiliation>George Washington University</affiliation>
  </overall_official>
  <overall_contact>
    <last_name>Lynne E Bernstein, Ph.D.</last_name>
    <phone>2029947403</phone>
    <email>lynneebernstein@gmail.com</email>
  </overall_contact>
  <overall_contact_backup>
    <last_name>Silvio P Eberhardt, Ph.D.</last_name>
    <phone>4845576593</phone>
    <email>seberhardt@ablazesystems.com</email>
  </overall_contact_backup>
  <location>
    <facility>
      <name>George Washington University</name>
      <address>
        <city>Washington</city>
        <state>Virginia</state>
        <zip>20052</zip>
        <country>United States</country>
      </address>
    </facility>
    <status>Recruiting</status>
    <contact>
      <last_name>Lynne E Bernstein, Ph.D.</last_name>
      <phone>202-994-7403</phone>
      <email>lbernste@gwu.edu</email>
    </contact>
  </location>
  <location_countries>
    <country>United States</country>
  </location_countries>
  <verification_date>August 2021</verification_date>
  <study_first_submitted>August 2, 2021</study_first_submitted>
  <study_first_submitted_qc>August 18, 2021</study_first_submitted_qc>
  <study_first_posted type="Actual">August 19, 2021</study_first_posted>
  <last_update_submitted>August 18, 2021</last_update_submitted>
  <last_update_submitted_qc>August 18, 2021</last_update_submitted_qc>
  <last_update_posted type="Actual">August 19, 2021</last_update_posted>
  <responsible_party>
    <responsible_party_type>Sponsor</responsible_party_type>
  </responsible_party>
  <keyword>lipreading speechreading</keyword>
  <keyword>perceptual learning</keyword>
  <keyword>adults with acquired hearing loss</keyword>
  <keyword>multisensory speech processing</keyword>
  <keyword>remote training system</keyword>
  <keyword>speech recognition in noise</keyword>
  <keyword>perceptual feedback during training</keyword>
  <condition_browse>
    <!-- CAUTION:  The following MeSH terms are assigned with an imperfect algorithm            -->
    <mesh_term>Hearing Loss</mesh_term>
    <mesh_term>Deafness</mesh_term>
  </condition_browse>
  <patient_data>
    <sharing_ipd>Yes</sharing_ipd>
    <ipd_description>De-identified data may be shared with other researchers following appropriate agreements. Personally-identifiable data will never be shared with individuals outside the research team, except as required by Law, NIH, or Geourge Washington University.</ipd_description>
    <ipd_info_type>Study Protocol</ipd_info_type>
    <ipd_info_type>Statistical Analysis Plan (SAP)</ipd_info_type>
    <ipd_info_type>Informed Consent Form (ICF)</ipd_info_type>
    <ipd_time_frame>Data will be available after they have been described in publications by the authors.</ipd_time_frame>
    <ipd_access_criteria>The data will be transferred via the Open Science Framework.</ipd_access_criteria>
    <ipd_url>http://osf.org</ipd_url>
  </patient_data>
  <provided_document_section>
    <provided_document>
      <document_type>Informed Consent Form</document_type>
      <document_has_protocol>No</document_has_protocol>
      <document_has_icf>Yes</document_has_icf>
      <document_has_sap>No</document_has_sap>
      <document_date>February 26, 2021</document_date>
      <document_url>https://ClinicalTrials.gov/ProvidedDocs/74/NCT05012774/ICF_000.pdf</document_url>
    </provided_document>
  </provided_document_section>
  <!-- Results have not yet been posted for this study                                          -->
</clinical_study>


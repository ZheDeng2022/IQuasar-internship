<clinical_study>
  <!-- This xml conforms to an XML Schema at:
    https://clinicaltrials.gov/ct2/html/images/info/public.xsd -->
  <required_header>
    <download_date>ClinicalTrials.gov processed this data on November 19, 2021</download_date>
    <link_text>Link to the current ClinicalTrials.gov record.</link_text>
    <url>https://clinicaltrials.gov/show/NCT05010473</url>
  </required_header>
  <id_info>
    <org_study_id>STUDY19070103</org_study_id>
    <secondary_id>R01DC013315</secondary_id>
    <nct_id>NCT05010473</nct_id>
  </id_info>
  <brief_title>Cortical Contributions to Frequency Following Responses and Modulation</brief_title>
  <official_title>Cortical Contributions to Frequency Following Responses and Modulation</official_title>
  <sponsors>
    <lead_sponsor>
      <agency>University of Pittsburgh</agency>
      <agency_class>Other</agency_class>
    </lead_sponsor>
    <collaborator>
      <agency>National Institute on Deafness and Other Communication Disorders (NIDCD)</agency>
      <agency_class>NIH</agency_class>
    </collaborator>
  </sponsors>
  <source>University of Pittsburgh</source>
  <oversight_info>
    <has_dmc>No</has_dmc>
    <is_fda_regulated_drug>No</is_fda_regulated_drug>
    <is_fda_regulated_device>No</is_fda_regulated_device>
  </oversight_info>
  <brief_summary>
    <textblock>
      The frequency-following response (FFR), a scalp-recorded neurophonic potential, is a widely&#xD;
      used metric of speech encoding integrity in healthy and clinical human populations. The&#xD;
      translational potential of the FFR as a biomarker is constrained by poor understanding of its&#xD;
      neural generators and influencing factors. This study leverages a cross-species and&#xD;
      cross-level approach to provide mechanistic insight into the properties of the cortical&#xD;
      source of the FFR, and elucidate the role of cortical feedback via cortico-collicular&#xD;
      projections on modulation of the FFR as a function of stimulus context, arousal state, and&#xD;
      category relevance. This clinical trial will focus on the influences of category relevance,&#xD;
      predictability, and participant arousal state on the FFRs in neurotypical human participants.&#xD;
    </textblock>
  </brief_summary>
  <detailed_description>
    <textblock>
      Participants will listen to a variety of sounds while the frequency-following response (FFR)&#xD;
      is recorded. The FFR is a sound-evoked response that mirrors the acoustic properties of the&#xD;
      incoming acoustic signal with remarkable fidelity. The FFR is now recognized as an integrated&#xD;
      response resulting from an interplay of early auditory subcortical and cortical systems. The&#xD;
      cortical dynamics underlying the FFR are unclear. All stimuli will be normalized to the same&#xD;
      root mean squared amplitude and stimulus duration and played at the same in-the-ear&#xD;
      intensity. Thirty-two stimuli from human and animal natural productions with a wide-range of&#xD;
      F0 will be used to elicit the FFR. At least 1000 artifact-free trials will be collected for&#xD;
      every stimulus. Participants will sit in a quiet room (patients) or a sound-treated booth and&#xD;
      listen to sounds while electroencephalography (EEG) and pupillometry signals are continuously&#xD;
      acquired. EEG signals will be collected using Ag-AgCl scalp electrodes, with the active&#xD;
      electrode placed at the central zero (Cz) point, the reference at the right mastoid, and the&#xD;
      ground at the left mastoid. Contact impedence will be&lt; 5 kΩ for all electrodes for all&#xD;
      recording sessions, and responses will be recorded at a sampling rate of 25 kHz using Brain&#xD;
      Vision PyCorder 1.0.7 (Brain Products, Gilching, Germany). Alternating polarities of the&#xD;
      stimuli will be binaurally presented in sound field (identical to the animal protocols), with&#xD;
      an inter-stimulus interval jittered between 122 to 148ms. Participants will be instructed to&#xD;
      stay awake and refrain from making extraneous movements. EEG and pupil measures will allow&#xD;
      continuous monitoring of participant state. The order of blocks will be counterbalanced&#xD;
      across participants, and stimulus presentation will be controlled by E-Prime 2.0.10 software.&#xD;
      The electrophysiological data will be preprocessed with BrainVision Analyzer 2.0 (Brain&#xD;
      Products, Gilching, Germany), bandpass filtered (varies based on stimulus F0; 12 dB/octave,&#xD;
      zero phase-shift). The bandpass filter will approximately reflect the lower and upper limits&#xD;
      of phase-locking along the auditory pathway that contributes to the FFR (auditory cortex,&#xD;
      midbrain). Responses will be segmented into epochs, baseline corrected to the mean voltage of&#xD;
      the noise floor(-40 to 0ms). Epochs in which the amplitude exceeds ± 35μV will be considered&#xD;
      artifacts and rejected. In each session, a preset number of artifact-free FFR trials (500 for&#xD;
      each polarity) will be obtained. For pupillometry, participants will be seated in a chair and&#xD;
      will place their head on a chin rest. The stabilized mount also has a small horizontal bar&#xD;
      that they can place their forehead against. To calibrate the eye tracker, participants will&#xD;
      be asked to follow 9 dots that appear on a monitor with their eyes. Insert earbuds will be&#xD;
      placed in both ears, and auditory stimuli will be presented binaurally. Pupillometry data&#xD;
      will be preprocessed to remove noise from the analysis. The pupil data will be downsampled to&#xD;
      50 Hz. Trials with more than 15% of the samples detected as blinks will be removed. Missing&#xD;
      samples due to blinks will be linearly interpolated from approximately100 ms prior to and 100&#xD;
      ms after the blink. Pupil responses will be baseline normalized using the average pupil size&#xD;
      in the 500-1000 ms prior to the onset of the auditory stimuli. A key variable reported will&#xD;
      be the percent change in pupil size.&#xD;
    </textblock>
  </detailed_description>
  <overall_status>Not yet recruiting</overall_status>
  <start_date type="Anticipated">December 2021</start_date>
  <completion_date type="Anticipated">January 2026</completion_date>
  <primary_completion_date type="Anticipated">January 2026</primary_completion_date>
  <phase>N/A</phase>
  <study_type>Interventional</study_type>
  <has_expanded_access>No</has_expanded_access>
  <study_design_info>
    <allocation>N/A</allocation>
    <intervention_model>Single Group Assignment</intervention_model>
    <primary_purpose>Basic Science</primary_purpose>
    <masking>None (Open Label)</masking>
  </study_design_info>
  <primary_outcome>
    <measure>Pitch tracking and decoding accuracies for dynamically varying pitch in naturally produced Mandarin tone categories</measure>
    <time_frame>During EEG session, up to 3 hours</time_frame>
    <description>The similarity between the stimulus and the FFR will be evaluated. The pitch tracking ability will be estimated by comparing the sliding window autocorrelation pitch tracks of the stimulus and FFR. Machine learning models will be trained to decode FFR to different stimulus categories based on pitch height and pitch direction. The chance estimates of the decoding accuracies will be estimated.</description>
  </primary_outcome>
  <primary_outcome>
    <measure>Stimulus-specific effects in Chinese vs. English participants</measure>
    <time_frame>During EEG session, up to 3 hours</time_frame>
    <description>The stimulus-specific effects in human participants, based on pitch tracking and decoding accuracies, will be assessed. The decoding accuracies and pitch tracking metrics for mandarin tone categories will be compared between Chinese listeners and English listeners. The focus will be to evaluate the specific benefits in Chinese listeners to track pitch height and direction patterns in Mandarin tones.</description>
  </primary_outcome>
  <primary_outcome>
    <measure>Arousal state and predictability effects on the FFRs</measure>
    <time_frame>During simultaneous EEG and pupillometry session, up to 3 hours</time_frame>
    <description>Arousal state and its dependence on encoding of predictability effects on the FFRs will be evaluated. The pitch tracking metrics and decoding accuracies will be compared across repetitive and contextual stimulus sequences while sorting the trials based on peak pupillary dilation as a proxy for arousal state. The predictability effects will be charted as a function of the arousal states and evaluated.</description>
  </primary_outcome>
  <number_of_arms>1</number_of_arms>
  <enrollment type="Anticipated">70</enrollment>
  <condition>Language</condition>
  <arm_group>
    <arm_group_label>Neurotypical Human Participants</arm_group_label>
    <arm_group_type>Experimental</arm_group_type>
    <description>Native speakers of Chinese and native speakers of English</description>
  </arm_group>
  <intervention>
    <intervention_type>Behavioral</intervention_type>
    <intervention_name>Speech and non-speech sound stimulation</intervention_name>
    <description>Listening to repetitive and extended sound presentation induces specific neural and physiological activity that we will measure via electroencephalography and pupillometry</description>
    <arm_group_label>Neurotypical Human Participants</arm_group_label>
  </intervention>
  <eligibility>
    <criteria>
      <textblock>
        Inclusion Criteria:&#xD;
&#xD;
          -  Age 13 years up to 25 years&#xD;
&#xD;
          -  Healthy volunteers&#xD;
&#xD;
          -  Native English speaking individuals with no exposure or experience to tonal languages&#xD;
&#xD;
          -  Native Mandarin speakers&#xD;
&#xD;
          -  Hearing sensitivity within normal limits (Puretone hearing thresholds &lt; 25dB from 250&#xD;
&#xD;
          -  Hz to 8000 Hz)&#xD;
&#xD;
          -  Less than six years of formal music training or experience&#xD;
&#xD;
        Exclusion Criteria:&#xD;
&#xD;
          -  Proficiency in languages other than English or Chinese&#xD;
&#xD;
          -  History of or current complaint of hearing loss or tinnitus&#xD;
&#xD;
          -  History of or current complaint of cognitive impairments&#xD;
&#xD;
          -  Individuals with more than 5 years of formal music experience or training&#xD;
&#xD;
          -  Complaints of impaired speech perception in noise&#xD;
      </textblock>
    </criteria>
    <gender>All</gender>
    <minimum_age>13 Years</minimum_age>
    <maximum_age>25 Years</maximum_age>
    <healthy_volunteers>Accepts Healthy Volunteers</healthy_volunteers>
  </eligibility>
  <overall_official>
    <last_name>Bharath Chandrasekaran, PhD</last_name>
    <role>Principal Investigator</role>
    <affiliation>University of Pittsburgh</affiliation>
  </overall_official>
  <overall_contact>
    <last_name>Bharath Chandrasekaran, PhD</last_name>
    <phone>412-383-0631</phone>
    <email>b.chandra@pitt.edu</email>
  </overall_contact>
  <location>
    <facility>
      <name>University of Pittsburgh</name>
      <address>
        <city>Pittsburgh</city>
        <state>Pennsylvania</state>
        <zip>15260</zip>
        <country>United States</country>
      </address>
    </facility>
    <contact>
      <last_name>Bharath Chandrasekaran, PhD</last_name>
      <phone>412-383-0631</phone>
      <email>b.chandra@pitt.edu</email>
    </contact>
    <investigator>
      <last_name>Bharath Chandrasekaran, PhD</last_name>
      <role>Principal Investigator</role>
    </investigator>
  </location>
  <location_countries>
    <country>United States</country>
  </location_countries>
  <verification_date>November 2021</verification_date>
  <study_first_submitted>July 30, 2021</study_first_submitted>
  <study_first_submitted_qc>August 10, 2021</study_first_submitted_qc>
  <study_first_posted type="Actual">August 18, 2021</study_first_posted>
  <last_update_submitted>November 1, 2021</last_update_submitted>
  <last_update_submitted_qc>November 1, 2021</last_update_submitted_qc>
  <last_update_posted type="Actual">November 3, 2021</last_update_posted>
  <responsible_party>
    <responsible_party_type>Principal Investigator</responsible_party_type>
    <investigator_affiliation>University of Pittsburgh</investigator_affiliation>
    <investigator_full_name>Bharath Chandrasekaran</investigator_full_name>
    <investigator_title>Professor</investigator_title>
  </responsible_party>
  <patient_data>
    <sharing_ipd>Yes</sharing_ipd>
    <ipd_description>We will follow the guidelines set forth by the Open Knowledge International, which is a global non-profit organization that advocates for open science and open data. Our objectives in data sharing are to provide free and open access to everyone. Consistent with ideas articulated in the Open Data Handbook (http://opendatahandbook.org/), we will apply for an open license for resources that are not deemed as intellectual properties, make data easily available in a format that is broadly accessible and ensures longevity, and advertise it on our webpages and via social media to make it discoverable.</ipd_description>
    <ipd_info_type>Study Protocol</ipd_info_type>
    <ipd_info_type>Statistical Analysis Plan (SAP)</ipd_info_type>
    <ipd_info_type>Analytic Code</ipd_info_type>
    <ipd_time_frame>Data will become available as soon as possible but no later than one year upon completion of the study</ipd_time_frame>
    <ipd_access_criteria>Our data will be made publicly available as soon as possible online to make it easily and widely accessible.</ipd_access_criteria>
  </patient_data>
  <!-- Results have not yet been posted for this study                                          -->
</clinical_study>


<clinical_study>
  <!-- This xml conforms to an XML Schema at:
    https://clinicaltrials.gov/ct2/html/images/info/public.xsd -->
  <required_header>
    <download_date>ClinicalTrials.gov processed this data on November 19, 2021</download_date>
    <link_text>Link to the current ClinicalTrials.gov record.</link_text>
    <url>https://clinicaltrials.gov/show/NCT04479761</url>
  </required_header>
  <id_info>
    <org_study_id>20-0312</org_study_id>
    <nct_id>NCT04479761</nct_id>
  </id_info>
  <brief_title>Sensory Integration of Auditory and Visual Cues in Diverse Contexts</brief_title>
  <official_title>Sensory Integration of Auditory and Visual Cues in Diverse Contexts Given Age, Vestibular Hypofunction and Hearing Loss</official_title>
  <sponsors>
    <lead_sponsor>
      <agency>New York University</agency>
      <agency_class>Other</agency_class>
    </lead_sponsor>
    <collaborator>
      <agency>The New York Eye &amp; Ear Infirmary</agency>
      <agency_class>Other</agency_class>
    </collaborator>
  </sponsors>
  <source>New York University</source>
  <oversight_info>
    <has_dmc>No</has_dmc>
    <is_fda_regulated_drug>No</is_fda_regulated_drug>
    <is_fda_regulated_device>No</is_fda_regulated_device>
  </oversight_info>
  <brief_summary>
    <textblock>
      More than 1/3 of adults in the United States seek medical attention for vestibular disorders&#xD;
      and hearing loss; disorders that can triple one's fall risk and have a profound effect on&#xD;
      one's participation in activities of daily living. Hearing loss has been shown to reduce&#xD;
      balance performance and could be one modifiable risk factor for falls. Patients with&#xD;
      vestibular hypofunction tend to avoid busy, hectic, visually complex, and loud environments&#xD;
      because these environments provoke dizziness and imbalance. While the visual impact on&#xD;
      balance is well known, less is known about the importance of sounds. In search for a possible&#xD;
      mechanism to explain a relationship between hearing and balance control, some studies&#xD;
      suggested that sounds may serve as an auditory anchor, providing spatial cues for balance,&#xD;
      similar to vision. However, the majority of these studies tested healthy adults' response to&#xD;
      sounds with blocked visuals. It is also possible that a relationship between hearing loss and&#xD;
      balance problems is navigated via an undetected vestibular deficit. By understanding the role&#xD;
      of auditory input in balance control, falls may be prevented in people with vestibular&#xD;
      disorders and hearing loss. Therefore, there is a critical need for a systematic&#xD;
      investigation of balance performance in response to simultaneous visual and auditory&#xD;
      perturbations, similar to real-life situations.&#xD;
&#xD;
      To answer this need, the investigators used recent advances in virtual reality technology and&#xD;
      developed a Head Mounted Display (HMD) protocol of immersive environments, combining specific&#xD;
      manipulations of visuals and sounds, including generated sounds (i.e., white noise) and&#xD;
      real-world recorded sounds (e.g., a train approaching a station). This research will answer&#xD;
      the following questions: (1) Are sounds used for balance and if yes, via what mechanism? (2)&#xD;
      Do individuals with single-sided hearing loss have a balance problem even without any&#xD;
      vestibular issues? (3) Are those with vestibular loss destabilized by sounds? To address&#xD;
      these questions, the following specific aims will be investigated in individuals with&#xD;
      unilateral peripheral vestibular hypofunction (n=45), individuals with single-sided deafness&#xD;
      (n=45), and age-matched controls (n=45): Aim 1: Establish the role of generated and natural&#xD;
      sounds in postural control in different visual environments; Aim 2: Determine the extent to&#xD;
      which a static white noise can improve balance within a dynamic visual environment.&#xD;
    </textblock>
  </brief_summary>
  <detailed_description>
    <textblock>
      Introduction: Aim 1 is to establish the role of generated and natural sounds in postural&#xD;
      control given the visual environment and sensory loss. For that the investigators will&#xD;
      measure postural sway in individuals with unilateral peripheral vestibular hypofunction&#xD;
      (n=45), individuals with SSD (n=45) and age-matched controls (n=45). They will be tested in&#xD;
      an immersive virtual reality environment displaying an abstract 3-wall display of stars or a&#xD;
      subway station. Within each environment, we will compare changes in postural sway in response&#xD;
      to visual (static, dynamic) and auditory perturbations (no sound, dynamic sound, i.e.,&#xD;
      rhythmic white noise in the stars environment or natural sounds, such as moving trains, in&#xD;
      the subway environment). Aim 2 is to determine the extent to which a static white noise can&#xD;
      improve balance (reduce postural sway) within a dynamic visual environment in individuals&#xD;
      with and without sensory loss. To accomplish this aim, the 3 groups of participants will be&#xD;
      tested within the same visual environment but here we will compare their sway within a&#xD;
      sound-free dynamic visual environment to that with static white noise.&#xD;
&#xD;
      System: Visuals were designed in C# language using standard Unity Engine version&#xD;
      2018.1.8f1(64-bit) (©Unity Tech., San Francisco, CA, USA). The scenes will be delivered via&#xD;
      an HTC Vive headset (Taoyuan City, Tai-wan) controlled by a Dell Alienware laptop 15 R3&#xD;
      (Round Rock, TX, USA). The Vive has built-in positional track-ing operating at 60Hz and a&#xD;
      refresh rate at 90 Hz. Sounds will be delivered via Bose (Bose Corporation, Fram-ingham, MA,&#xD;
      USA) QuietComfort 35 wireless headphones II with active noise cancellation and 360º spatial&#xD;
      audio. The process of creating auditory cues included over 20 hours of sound field recording&#xD;
      based on the targeted scenes and their intensity levels in New York City. Auditory cues were&#xD;
      captured with the Sennheiser Ambeo microphone in first order Ambisonics format. The&#xD;
      background sounds merged with a sound design process which involved simulating the detailed&#xD;
      environmental sounds that exist within the natural environment to develop a real-world sonic&#xD;
      representation. The audio files were processed in Wwise and integrated into Unity. Postural&#xD;
      sway will be recorded at 100 Hz by Qualisys software for a Kistler 5233A force-platform&#xD;
      (Winterthur, Switzer-land).&#xD;
&#xD;
      Data Collection: Potentially eligible participants will complete a demographics form and go&#xD;
      through the following diagnostic screening at the Ear Institute: Caloric Test, Video Head&#xD;
      Impulse Test (vHIT), Ocular / Cervical Vestibular Evoked Myogenic Potential, and Audiogram.&#xD;
      Visual and somatosensory screening will be done at the Ear Institute as well. This first&#xD;
      session is expected to take 2.5 hours to complete. Participants will receive questionnaires&#xD;
      to complete at home or on the next session. The Dizziness Handicap Inventory (DHI) was&#xD;
      designed to identify difficulties that a patient may be experiencing because of dizziness.&#xD;
      The Activities-Specific Balance Confidence (ABC) is a measure of confidence in performing&#xD;
      various ambulatory activities without falling or feeling 'unsteady'. The State-Trait Anxiety&#xD;
      Inventory (STAI) assesses the severity of anxiety symptoms and a generalized tendency to be&#xD;
      anxious. The Speech, Spatial and Quality of Hearing 12-item Scale (SSQ12) is a valid, short&#xD;
      version of the original SSQ which provides insights on day-to-day hearing loss impact. The&#xD;
      virtual reality protocol (testing by the PI at the NYU Human Performance Laboratory) includes&#xD;
      12 conditions: 2 environments (an abstract display of stars, a subway station) X 2 visuals&#xD;
      (moving, static) X 3 sounds (dynamic, none, static white noise) each repeated 3 times for a&#xD;
      total of 36 trials. It will be randomized and completed over 1-2 sessions, as needed of up to&#xD;
      90 minutes each. Sounds will be played at the highest level that is comfortable to the&#xD;
      participant. Scenes are 60 seconds long. Throughout all sessions, the patients will complete&#xD;
      the Simulator Sickness Questionnaire, used to monitor participants' symptoms.&#xD;
&#xD;
      Data Analysis: For each of the 3 measures of interest and for each environment, we will fit a&#xD;
      linear mixed effects model. Each model will include main effects of group, visual condition,&#xD;
      and auditory condition, as well as all 2 and 3-way interactions. The models will also control&#xD;
      for caloric and Video Head Impulse Test (vHIT) test results as well as Age Related Hearing&#xD;
      Loss and age. For aim 1, we will assess the significance of contrasts between no sounds /&#xD;
      dynamic sounds for the different visual conditions and groups. For aim 2, the same will be&#xD;
      done for contrasts between no sounds / static sounds. These models estimate the difference in&#xD;
      visual weighting and reweighting between the groups, maximizing the information we can obtain&#xD;
      from the data by accounting for the inherent multi-level study design (person, conditions,&#xD;
      repetitions). Since each person completes various trials for each condition, the linear mixed&#xD;
      effects model accounts for these sources of variability. P-values for the fixed effects will&#xD;
      be calculated using the Satterthwaite approximation for the degrees of freedom for the&#xD;
      T-distribution80. In addition, we will descriptively explore the relationship between DP,&#xD;
      area, self-reported outcomes (DHI, ABC, STAI, SSQ12), and age.&#xD;
    </textblock>
  </detailed_description>
  <overall_status>Not yet recruiting</overall_status>
  <start_date type="Anticipated">October 1, 2021</start_date>
  <completion_date type="Anticipated">August 31, 2023</completion_date>
  <primary_completion_date type="Anticipated">April 15, 2023</primary_completion_date>
  <phase>N/A</phase>
  <study_type>Interventional</study_type>
  <has_expanded_access>No</has_expanded_access>
  <study_design_info>
    <allocation>N/A</allocation>
    <intervention_model>Single Group Assignment</intervention_model>
    <intervention_model_description>We will have 3 groups: people with unilateral vestibular loss, people with single-sided hearing loss and controls. All groups will go through the same procedure.</intervention_model_description>
    <primary_purpose>Diagnostic</primary_purpose>
    <masking>None (Open Label)</masking>
  </study_design_info>
  <primary_outcome>
    <measure>Sway Directional Path (DP) anterior-posterior [AP] and medio-lateral [ML] in mm</measure>
    <time_frame>Will be measured during all 60 seconds scenes</time_frame>
    <description>Total path length of the position curve for a selected direction. DP is a measure of postural steadiness and is used as an indication of how much static balance was perturbed with a given sensory manipulation.</description>
  </primary_outcome>
  <primary_outcome>
    <measure>Sway Area (mm2)</measure>
    <time_frame>Will be measured during all 60 seconds scenes</time_frame>
    <description>confidence ellipse surface containing 95% of all center-of-pressure positions.</description>
  </primary_outcome>
  <number_of_arms>1</number_of_arms>
  <enrollment type="Anticipated">135</enrollment>
  <condition>Vestibular Disorder</condition>
  <condition>Hearing Loss, Sensorineural</condition>
  <arm_group>
    <arm_group_label>Virtual Reality</arm_group_label>
    <arm_group_type>Experimental</arm_group_type>
    <description>Participants will be wearing a virtual reality headset and observing 2 types of scenes: abstract (a display of stars) or contextual (a subway station).</description>
  </arm_group>
  <intervention>
    <intervention_type>Behavioral</intervention_type>
    <intervention_name>Visual and Auditory Cues</intervention_name>
    <description>Within each scene there will be 2 levels of visual input (static or dynamic) combined with 3 levels of sounds (static, none or dynamic). Postural responses to each combination will be evaluated in order to assess the role of generated and natural sounds in postural control and whether static sounds can improve balance within dynamic virtual environments.</description>
    <arm_group_label>Virtual Reality</arm_group_label>
  </intervention>
  <eligibility>
    <criteria>
      <textblock>
        Inclusion Criteria:&#xD;
&#xD;
        Group 1: Unilateral peripheral vestibular hypofunction and normal hearing, e.g., vestibular&#xD;
        neuritis.&#xD;
&#xD;
        a complaint of head motion provoked instability or dizziness affecting their functional&#xD;
        mobility and quality of life at least 1 positive finding indicating unilateral vestibular&#xD;
        hypofunction on the following clinical tests: head thrust, subjective visual vertical and&#xD;
        horizontal, post head shaking nystagmus, spontaneous and gaze holding nystagmus a score of&#xD;
        at least 16 (mild handicap) on the Dizziness Handicap Inventory (DHI).&#xD;
&#xD;
        meeting at least 1 of the following diagnostic criteria: 25% or above unilateral weakness&#xD;
        on caloric testing; Low gain on Video Head Impulse Test (vHIT) &lt;.8; Ocular Vestibular&#xD;
        evoked myogenic potential (oVemp) amplitude asymmetry greater than 34%; Cervical (cVemp)&#xD;
        amplitude asymmetry greater than 40%. Normal hearing, defined as an unaided PTA &lt; 26dB HL&#xD;
        (0.5-4 kHz) bilaterally.&#xD;
&#xD;
        Group 2: Acquired severe / profound unilateral hearing loss (i.e., single-sided deafness&#xD;
        [SSD]), no evidence of retrocochlear pathology on MRI and no active complaint of dizziness&#xD;
        (DHI score &lt; 10) or imbalance. SSD will be defined as having an unaided pure-tone average&#xD;
        (PTA) of hearing thresholds at 0.5, 1, 2, and 4 kHz in the affected ear &gt; 70 dB HL and&#xD;
        normal hearing in the contralateral ear. Normal hearing will be defined as an unaided PTA &lt;&#xD;
        26dB HL (0.5-4 kHz). This is considered healthy hearing according to the World Health&#xD;
        Organization.&#xD;
&#xD;
        Group 3: Healthy controls who are matched for age and sex with group 1.&#xD;
&#xD;
        For those above 65 years of age, symmetric age-related hearing loss (ARHL) in the mild&#xD;
        hearing loss range, specifically an unaided PTA &lt; 40 dB (0.5-4KHz) will be included.&#xD;
&#xD;
        Exclusion Criteria:&#xD;
&#xD;
        a medical diagnosis of peripheral neuropathy; lack of protective sensation based on the&#xD;
        Semmes-Weinstein 5.07 Monofilament Test; conductive hearing loss or air bone gap; visual&#xD;
        impairment above 20/63 (NYS Department of Motor Vehicle cutoff for driving) on the Early&#xD;
        Treatment Diabetic Retinopathy Study (ETDRS) Acuity Test that cannot be corrected with&#xD;
        lenses; pregnancy; any neurological condition interfering with balance or walking (e.g.&#xD;
        multiple sclerosis, Parkinson's disease, stroke); acute musculoskeletal pain at time of&#xD;
        testing; currently seeking medical care for another orthopaedic condition; inability to&#xD;
        read an informed consent in English, Spanish or Chinese. Control participants will be&#xD;
        excluded for any positive finding on the vestibular diagnostic testing or history of&#xD;
        vestibular symptoms (dizziness, vertigo) or any hearing loss that does not fit ARHL as per&#xD;
        the criteria specified above.&#xD;
&#xD;
        Patients with vestibular hypofunction will be excluded if they are diagnosed with an&#xD;
        unstable peripheral lesion, e.g., Meniere's Disease, Perilymphatic Fistula, Superior Canal&#xD;
        Dehiscence, or Acoustic Neuroma.&#xD;
      </textblock>
    </criteria>
    <gender>All</gender>
    <minimum_age>18 Years</minimum_age>
    <maximum_age>N/A</maximum_age>
    <healthy_volunteers>Accepts Healthy Volunteers</healthy_volunteers>
  </eligibility>
  <overall_official>
    <last_name>Anat V Lubetzky, PhD</last_name>
    <role>Principal Investigator</role>
    <affiliation>New York University</affiliation>
  </overall_official>
  <overall_contact>
    <last_name>Anat V Lubetzky, PhD</last_name>
    <phone>212-998-9195</phone>
    <email>anat@nyu.edu</email>
  </overall_contact>
  <location>
    <facility>
      <name>New York Eye and Ear Infirmary of Mount Sinai</name>
      <address>
        <city>New York</city>
        <state>New York</state>
        <zip>10010</zip>
        <country>United States</country>
      </address>
    </facility>
  </location>
  <location>
    <facility>
      <name>New York University Physical Therapy Department</name>
      <address>
        <city>New York</city>
        <state>New York</state>
        <zip>10010</zip>
        <country>United States</country>
      </address>
    </facility>
    <contact>
      <last_name>Anat V Lubetzky, PT, PhD</last_name>
      <phone>212-998-9195</phone>
      <email>anat@nyu.edu</email>
    </contact>
  </location>
  <location_countries>
    <country>United States</country>
  </location_countries>
  <verification_date>March 2021</verification_date>
  <study_first_submitted>July 16, 2020</study_first_submitted>
  <study_first_submitted_qc>July 16, 2020</study_first_submitted_qc>
  <study_first_posted type="Actual">July 21, 2020</study_first_posted>
  <last_update_submitted>September 28, 2021</last_update_submitted>
  <last_update_submitted_qc>September 28, 2021</last_update_submitted_qc>
  <last_update_posted type="Actual">September 29, 2021</last_update_posted>
  <responsible_party>
    <responsible_party_type>Sponsor</responsible_party_type>
  </responsible_party>
  <condition_browse>
    <!-- CAUTION:  The following MeSH terms are assigned with an imperfect algorithm            -->
    <mesh_term>Hearing Loss</mesh_term>
    <mesh_term>Deafness</mesh_term>
    <mesh_term>Hearing Loss, Sensorineural</mesh_term>
    <mesh_term>Vestibular Diseases</mesh_term>
  </condition_browse>
  <patient_data>
    <sharing_ipd>No</sharing_ipd>
  </patient_data>
  <!-- Results have not yet been posted for this study                                          -->
</clinical_study>


<clinical_study>
  <!-- This xml conforms to an XML Schema at:
    https://clinicaltrials.gov/ct2/html/images/info/public.xsd -->
  <required_header>
    <download_date>ClinicalTrials.gov processed this data on November 19, 2021</download_date>
    <link_text>Link to the current ClinicalTrials.gov record.</link_text>
    <url>https://clinicaltrials.gov/show/NCT05004649</url>
  </required_header>
  <id_info>
    <org_study_id>305435</org_study_id>
    <secondary_id>K99NS118117</secondary_id>
    <nct_id>NCT05004649</nct_id>
  </id_info>
  <brief_title>Studying the Effects of Natural Visual Scene Changes on Typical Adult Visual Perception</brief_title>
  <official_title>The Effects of Stimulus Variability in Natural Visual Scenes</official_title>
  <sponsors>
    <lead_sponsor>
      <agency>University of Pennsylvania</agency>
      <agency_class>Other</agency_class>
    </lead_sponsor>
    <collaborator>
      <agency>National Institute of Neurological Disorders and Stroke (NINDS)</agency>
      <agency_class>NIH</agency_class>
    </collaborator>
  </sponsors>
  <source>University of Pennsylvania</source>
  <oversight_info>
    <has_dmc>No</has_dmc>
    <is_fda_regulated_drug>No</is_fda_regulated_drug>
    <is_fda_regulated_device>No</is_fda_regulated_device>
  </oversight_info>
  <brief_summary>
    <textblock>
      The natural visual environment is complex and rich with different stimuli and features. The&#xD;
      visual system must constantly extract behaviorally relevant visual information from an&#xD;
      abundance of irrelevant information in the visual scene. To complicate matters further, the&#xD;
      visual feature or stimulus that is most relevant at any given moment can change quickly and&#xD;
      frequently in realistic visual environments. The mechanisms by which task-relevant&#xD;
      information guides perceptual behavior are not fully understood.&#xD;
&#xD;
      In this study, psychophysical experiments will be used to measure participants' ability to&#xD;
      discriminate the horizontal position of a central object within a complex, natural visual&#xD;
      scene, as well as to measure how that ability is affected by within-trial variability in the&#xD;
      features of background objects in the scene.&#xD;
&#xD;
      The goal of this study is to investigate the overarching prediction that the visual system&#xD;
      extracts task-relevant information in a manner that reflects realistically complex visual&#xD;
      environments in which the stimuli change quickly and frequently. Specifically, this study&#xD;
      will test the hypothesis that task-irrelevant variability in the scene affects participants'&#xD;
      ability to discriminate the visual feature that is relevant to the task at hand.&#xD;
    </textblock>
  </brief_summary>
  <detailed_description>
    <textblock>
      Background&#xD;
&#xD;
      The natural visual environment is often complex, with a single visual scene containing a wide&#xD;
      variety of stimuli. These stimuli can change quickly and frequently. To further complicate&#xD;
      matters, the behavioral relevance of any particular stimulus or visual feature can also&#xD;
      change at any given moment. Visual perception in realistically complex visual environments&#xD;
      requires the constant extraction of task-relevant stimulus information from an abundance of&#xD;
      irrelevant information. Understanding how task-relevant information is used to guide behavior&#xD;
      in the context of constantly changing, feature-rich visual environments is a key component of&#xD;
      understanding perception. This may be particularly true if the visual system is&#xD;
      evolutionarily optimized to perform in realistically complex environments in which the&#xD;
      behaviorally relevant stimulus or feature can change quickly and frequently.&#xD;
&#xD;
      The goal of this study is to investigate the overarching prediction that the visual system is&#xD;
      optimized to extract visual information in a generalized manner that is flexible to the wide&#xD;
      variety of constantly changing visual features encountered in natural environments.&#xD;
      Specifically, this study will test the hypothesis that, in the context of realistically&#xD;
      complex natural scenes, task-irrelevant visual feature variability negatively affects&#xD;
      participants' ability to discriminate the visual feature that is relevant to the task at&#xD;
      hand.&#xD;
&#xD;
      Methods&#xD;
&#xD;
      Participants&#xD;
&#xD;
      The experimental protocols are approved by the University of Pennsylvania Institutional&#xD;
      Review Board. Participants will be invited to volunteer to participate in this study.&#xD;
      Participants will provide informed consent. To ensure that the participants meet the&#xD;
      eligibility criteria, prior to the experiment, they will be interviewed and they will fill&#xD;
      out a survey. Also prior to the experiment, they will be screened for visual acuity using a&#xD;
      Snellen eye chart and for color deficiencies using the Ishihara plate test. They will be&#xD;
      excluded prior to the experiment if their best-corrected visual acuity is worse than 20/40 in&#xD;
      either eye or if they make any errors on the Ishihara plate test.&#xD;
&#xD;
      For enrolled participants, their threshold for horizontal position discrimination (in a&#xD;
      control condition without any task-irrelevant variability; see Session organization below for&#xD;
      details) will be calculated based on their performance on the experimental task during their&#xD;
      first session. Participants will be excluded after the conclusion of their first session if&#xD;
      their horizontal position discrimination threshold in the control condition is higher than a&#xD;
      maximum value of 0.6 degrees of visual angle, and participants excluded at this point will&#xD;
      not participate in any further experimental sessions. If very few enrolled participants&#xD;
      satisfy this criterion, then this maximum threshold value for participant inclusion will be&#xD;
      increased. In the case that the maximum threshold value for inclusion is increased,&#xD;
      participants that had been previously excluded will not be re-included in the experiment&#xD;
      posthoc.&#xD;
&#xD;
      Apparatus&#xD;
&#xD;
      A calibrated LCD color monitor (27-inch NEC MultiSync PA271Q QHD Color Critical Desktop W-LED&#xD;
      Monitor with SpectraView Engine; NEC Display Solutions) will be used to display the stimuli&#xD;
      in an otherwise dark room, after participants have dark-adapted in the experimental room for&#xD;
      a minimum of 5 minutes. The monitor will be driven at a pixel resolution of 1920 x 1080, with&#xD;
      a refresh rate of 60 Hz and with 8-bit resolution for each RGB channel. The host computer for&#xD;
      this monitor will be an Apple Macintosh with an Intel Core i7 processor. The head position of&#xD;
      each participant will be stabilized using a chin cup (Headspot, UHCOTech, Houston, TX). The&#xD;
      participant's eyes will be centered horizontally and vertically with respect to the monitor.&#xD;
      The distance between the participant's eyes and the monitor will be 75 cm. The participant&#xD;
      will input their responses using a Logitech F310 gamepad controller.&#xD;
&#xD;
      Stimuli&#xD;
&#xD;
      All of the stimuli are variants of the same natural visual scene: a square image (subtending&#xD;
      8 degrees of visual angle in both width and height), in which a central object (a banana,&#xD;
      subtending approximately 4 degrees of visual angle in height) is presented on an&#xD;
      approximately circular array of overlapping background objects (subtending approximately 5&#xD;
      degrees of visual angle and made up of overlapping branches and leaves). The central object&#xD;
      (the banana) and/or what will be referred to as the &quot;background objects&quot; (the branches and&#xD;
      leaves) change in horizontal position, rotation, and/or depth across different stimuli. The&#xD;
      central object and background objects are presented in the context of other objects that do&#xD;
      not ever move across different stimuli (a rock ledge, a skyline, and three moss-covered&#xD;
      stumps).&#xD;
&#xD;
      The natural visual scene was created using Blender, an open-source 3D creation suite&#xD;
      (https://www.blender.org, Version 2.81a). The central object and/or the background objects&#xD;
      were moved in horizontal position, rotation, and/or depth to create different stimuli using&#xD;
      iset3d, an open-source software package (https://github.com/ISET/iset3d) that works with a&#xD;
      modified version of pbrt (https://github.com/mmp/pbrt-v3). The images were created using&#xD;
      iset3d at a resolution of 1920 x 1920 with 100 samples per pixel, at 31 equally spaced&#xD;
      wavelengths between 400 nm and 700 nm.&#xD;
&#xD;
      The images created using iset3d were converted to RGB images using custom software (Natural&#xD;
      Image Thresholds; https://github.com/AmyMNi/NaturalImageThresholds) written using MATLAB&#xD;
      (MathWorks; Natick, MA) and based on the software package Virtual World Color Constancy&#xD;
      (github.com/BrainardLab/VirtualWorldColorConstancy). Natural Image Thresholds is dependent on&#xD;
      routines from the Psychophysics Toolbox (http://psychtoolbox.org), iset3d&#xD;
      (https://github.com/ISET/iset3d), and isetbio (http://psychtoolbox.org). To convert a&#xD;
      hyperspectral image created using iset3d to an RGB image for presentation on the calibrated&#xD;
      monitor, the hyperspectral image data were first used to compute LMS cone excitations. Then,&#xD;
      the LMS cone excitations were converted to a metameric rendered image in the RGB color space&#xD;
      of the monitor, based on the monitor calibration data. The RGB image was gamma corrected&#xD;
      using a common scaling that brought all of the RGB images in the stimulus set into the&#xD;
      display gamut of the monitor.&#xD;
&#xD;
      The stimuli will be presented on the calibrated monitor in the context of the psychophysical&#xD;
      task. The stimuli will be presented on a uniform gray background (~100 cd/m^2), which will be&#xD;
      presented on the monitor for the duration of the experimental session.&#xD;
&#xD;
      Psychophysical task&#xD;
&#xD;
      A psychophysical task will be used to measure participants' ability to discriminate the&#xD;
      horizontal position of the central object that is presented within the context of background&#xD;
      objects in a natural visual scene. The task will be a two-interval forced choice task that&#xD;
      presents one stimulus per interval. Each interval will have a duration of 250 ms. Each&#xD;
      stimulus will be presented at the center of the monitor. Between the two stimulus intervals,&#xD;
      two masks will be shown in succession at the center of the monitor. Each mask will be&#xD;
      presented for a duration of 400 ms, for a total interstimulus interval of 800 ms (see Session&#xD;
      organization below for mask details).&#xD;
&#xD;
      The task of the participant will be to determine whether, compared to the central object&#xD;
      presented in the first interval, the central object presented in the second interval is to&#xD;
      the left or to the right. Following the two intervals, the participant will have an unlimited&#xD;
      amount of time to press one of two response buttons on the gamepad (the upper left trigger to&#xD;
      indicate that the central object in the second interval was to the left, the upper right&#xD;
      trigger to indicate that it was to the right). One of two feedback tones will be presented&#xD;
      after the response is entered, indicating whether the participant was correct or incorrect.&#xD;
      For trials in which there is no difference in the position of the central object between the&#xD;
      two intervals, the response that will receive the correct feedback tone will be randomly&#xD;
      selected per trial. The trials will be separated by an intertrial interval of approximately 1&#xD;
      second.&#xD;
&#xD;
      The experimental programs can be found in the custom software package Natural Image&#xD;
      Thresholds (https://github.com/AmyMNi/NaturalImageThresholds). They were written in MATLAB&#xD;
      (MathWorks; Natick, MA) and were based on the software package Virtual World Color Constancy&#xD;
      (github.com/BrainardLab/VirtualWorldColorConstancy). They rely on routines from the&#xD;
      Psychophysics Toolbox (http://psychtoolbox.org) and mgl&#xD;
      (http://justingardner.net/doku.php/mgl/overview).&#xD;
&#xD;
      Session organization&#xD;
&#xD;
      It is expected that participants will complete this pilot experiment in six sessions. The&#xD;
      first session will include participant enrollment procedures (informed consent, vision tests,&#xD;
      etc.; see Participants above for details) as well as familiarization trials (see next&#xD;
      paragraph) and will last approximately one and a half hours. The second through sixth&#xD;
      sessions will last approximately one hour each.&#xD;
&#xD;
      In the first session, prior to beginning the task, the participant will be provided with task&#xD;
      instructions and will be given the opportunity to practice pressing the response buttons. For&#xD;
      the first session only, the participant will begin with 30 familiarization trials. The&#xD;
      familiarization trials will comprise, in order: 10 randomly selected easy trials (the largest&#xD;
      position-change comparisons), 10 randomly selected medium-difficulty trials (the 4th and 5th&#xD;
      largest position-change comparisons), and 10 randomly selected trials from all possible&#xD;
      position-change comparisons. The familiarization trials will not include any task-irrelevant&#xD;
      variability. Data from the familiarization trials will not be saved. The familiarization&#xD;
      trials will be followed by a break, during which the participant will be given the&#xD;
      opportunity to ask any questions they may have. The break will end when the participant&#xD;
      indicates that they are ready (using a button press), and the experiment will begin.&#xD;
&#xD;
      In each session, there will be two conditions: &quot;condition&quot; refers to the reference position&#xD;
      of the central object. For each reference position, there will be 11 &quot;comparison&quot; positions&#xD;
      for the central object: five comparison positions in the positive horizontal direction, five&#xD;
      comparison positions in the negative horizontal direction, and a comparison position of 0&#xD;
      indicating no change. In each trial, one interval will contain a reference stimulus and the&#xD;
      other interval will contain one of that reference stimulus's comparison stimuli. The order in&#xD;
      which these two stimuli are presented within a trial will be selected randomly per trial.&#xD;
&#xD;
      A &quot;block&quot; of trials will consist of 2 conditions and 11 comparisons per condition, for a&#xD;
      total of 22 trials. The trials within a block will be run in randomized order. A block will&#xD;
      be completed before the next block of trials begins. There will be 14 iterations of a block,&#xD;
      for a total of 308 trials.&#xD;
&#xD;
      This set of 308 trials will make up a single &quot;noise level&quot;. A single session will consist of&#xD;
      three noise levels: Noise Level 0, Noise Level 1, and Noise Level 2. The trials for each&#xD;
      noise level will be divided into two &quot;runs&quot; (154 trials per run). Thus, each run will&#xD;
      comprise a single noise level. The six runs will be run in random order per session. Each run&#xD;
      will be separated by a break that lasts at least one minute and during which the participant&#xD;
      will be instructed to stand or stretch as needed. Each break will end when the participant&#xD;
      indicates that they are ready (using a button press).&#xD;
&#xD;
      Across all six runs, there will be a total of 924 trials. Additionally, each session will&#xD;
      begin with four practice trials (including the first experimental session, which will be&#xD;
      preceded by familiarization trials as described above). Each session will also include one&#xD;
      practice trial following each of the five breaks. Each practice trial will be randomly&#xD;
      selected from the set of easy trials (described above) and will not include any&#xD;
      task-irrelevant variability. The data from the practice trials will not be saved. Including&#xD;
      the nine practice trials, there will be a total of 933 trials per session.&#xD;
&#xD;
      For Noise Level 0, there will not be any changes to the background objects (the branches and&#xD;
      leaves). Noise Level 0 will be the control condition and will be used to determine the&#xD;
      participant's threshold for discriminating the horizontal position of the central object&#xD;
      without any task-irrelevant stimulus noise. Noise Levels 1 and 2 will be used to determine&#xD;
      the participant's threshold for discriminating the horizontal position of the central object&#xD;
      in the presence of task-irrelevant stimulus noise.&#xD;
&#xD;
      Noise Level 1 will consist of task-irrelevant noise in a single task-irrelevant feature:&#xD;
      rotation. A task-irrelevant rotation amount will be applied to each stimulus separately. For&#xD;
      each stimulus, a single rotation amount will be drawn randomly from a pool of 51 rotation&#xD;
      amounts, and the background objects in the stimulus will all be rotated by that rotation&#xD;
      amount. The rotation amount will be drawn separately (randomly with replacement) for each of&#xD;
      the two stimuli presented in a trial (the reference position stimulus, and the comparison&#xD;
      position stimulus). The pool of 51 rotation amounts will comprise: one rotation amount of&#xD;
      zero (no change to the background objects), 25 equally spaced rotation amounts in the&#xD;
      clockwise direction, and 25 equally spaced rotation amounts in the counterclockwise&#xD;
      direction.&#xD;
&#xD;
      Noise Level 2 will consist of task-irrelevant noise in two task-irrelevant features: rotation&#xD;
      and depth. For Noise Level 2, there will also be a pool of 51 noise amounts, but each noise&#xD;
      amount in the pool will consist of both a rotation amount (the same 51 rotation amounts as in&#xD;
      Noise Level 1) and a depth amount. There will be 51 possible depth amounts (one depth amount&#xD;
      of zero, 25 equally spaced depth amounts in the position direction, and 25 equally spaced&#xD;
      depth amounts in the negative direction). For the Noise Level 2 pool of 51 noise amounts, one&#xD;
      of the noise amounts will consist of a rotation amount of zero and a depth amount of zero.&#xD;
      For the remaining 50 noise amounts in the pool, each of the remaining 50 rotation amounts&#xD;
      will be randomly assigned (without replacement) to one of the remaining 50 depth amounts.&#xD;
      From this Noise Level 2 pool of 51 noise amounts, a single noise amount will be randomly&#xD;
      drawn (with replacement) for each of the two stimuli in a trial separately.&#xD;
&#xD;
      Finally, as noted above (see Psychophysical task), two masks will be shown per trial during&#xD;
      the interstimulus interval. All masks across all noise levels will be drawn from the same&#xD;
      distribution of stimuli (stimuli with Noise Level 0, thus containing no task-irrelevant&#xD;
      noise). To create each of the two masks in a trial: first, the central object positions in&#xD;
      the first and second intervals of the trial will be determined. The two stimuli with Noise&#xD;
      Level 0 that match the central object positions in the first and second intervals will be&#xD;
      used to create the trial masks. For each of these two stimuli, the average intensity will be&#xD;
      calculated per RGB channel per 16 x 16 block of the stimulus. Next, each 16 x 16 block of a&#xD;
      mask will be randomly drawn from either one or the other stimulus, and will be made up of the&#xD;
      uniform average intensity calculated per RGB channel for that block of the selected stimulus.&#xD;
      Thus, the two masks shown per trial will each consist of a different random draw per 16 x 16&#xD;
      block.&#xD;
&#xD;
      Data Analysis&#xD;
&#xD;
      Per session, the participant's threshold for discriminating object position will be measured&#xD;
      for each noise level. First, for each comparison position, the proportion of trials on which&#xD;
      the participant responded that the comparison stimulus was located to the right of the&#xD;
      reference stimulus will be calculated. Next, the proportion the comparison was chosen as&#xD;
      rightwards will be fit with a cumulative normal function using the Palamedes Toolbox&#xD;
      (http://www.palamedestoolbox.org). To estimate all four parameters of the psychometric&#xD;
      function (threshold, slope, lapse rate, and guess rate), the lapse rate will be set equal to&#xD;
      the guess rate and will be forced to be in the range [0, 0.05], and the model will be fit to&#xD;
      the data using the maximum likelihood method. The threshold will be calculated as the&#xD;
      difference between the stimulus levels at performances (proportion the comparison was chosen&#xD;
      as rightwards) equal to 0.7602 and 0.5 as determined by the cumulative normal fit.&#xD;
    </textblock>
  </detailed_description>
  <overall_status>Recruiting</overall_status>
  <start_date type="Actual">August 9, 2021</start_date>
  <completion_date type="Anticipated">January 31, 2022</completion_date>
  <primary_completion_date type="Anticipated">January 31, 2022</primary_completion_date>
  <study_type>Observational</study_type>
  <has_expanded_access>No</has_expanded_access>
  <study_design_info>
    <observational_model>Case-Only</observational_model>
    <time_perspective>Prospective</time_perspective>
  </study_design_info>
  <primary_outcome>
    <measure>Psychophysical threshold measurements</measure>
    <time_frame>Approximately 3 weeks</time_frame>
    <description>Behavioral assays of psychophysical threshold measurements for discriminations of object position</description>
  </primary_outcome>
  <enrollment type="Anticipated">10</enrollment>
  <condition>Visual Perception</condition>
  <eligibility>
    <study_pop>
      <textblock>
        The subject population for this study will be drawn primarily from the University of&#xD;
        Pennsylvania community and the surrounding Philadelphia community.&#xD;
      </textblock>
    </study_pop>
    <sampling_method>Non-Probability Sample</sampling_method>
    <criteria>
      <textblock>
        Inclusion Criteria:&#xD;
&#xD;
          -  Normal visual acuity&#xD;
&#xD;
          -  Capable of giving informed consent&#xD;
&#xD;
          -  Fully vaccinated against COVID-19&#xD;
&#xD;
        Exclusion Criteria:&#xD;
&#xD;
          -  Known color deficiencies&#xD;
&#xD;
          -  Diagnosis of retinal disease or inherited retinal disease from family history&#xD;
&#xD;
          -  A psychophysical threshold for horizontal position discrimination that is greater than&#xD;
             0.6 degrees of visual angle (to be determined during the first experimental session)&#xD;
      </textblock>
    </criteria>
    <gender>All</gender>
    <minimum_age>18 Years</minimum_age>
    <maximum_age>N/A</maximum_age>
    <healthy_volunteers>Accepts Healthy Volunteers</healthy_volunteers>
  </eligibility>
  <overall_official>
    <last_name>Amy M. Ni, Ph.D.</last_name>
    <role>Principal Investigator</role>
    <affiliation>University of Pennsylvania</affiliation>
  </overall_official>
  <overall_official>
    <last_name>David H. Brainard, Ph.D.</last_name>
    <role>Study Director</role>
    <affiliation>University of Pennsylvania</affiliation>
  </overall_official>
  <overall_contact>
    <last_name>Amy M. Ni, Ph.D.</last_name>
    <phone>412-268-3922</phone>
    <email>amyni@sas.upenn.edu</email>
  </overall_contact>
  <location>
    <facility>
      <name>University of Pennsylvania</name>
      <address>
        <city>Philadelphia</city>
        <state>Pennsylvania</state>
        <zip>19104</zip>
        <country>United States</country>
      </address>
    </facility>
    <status>Recruiting</status>
    <contact>
      <last_name>Amy M. Ni, Ph.D.</last_name>
      <phone>412-268-3922</phone>
      <email>amyni@sas.upenn.edu</email>
    </contact>
  </location>
  <location_countries>
    <country>United States</country>
  </location_countries>
  <reference>
    <citation>Brainard DH. The Psychophysics Toolbox. Spat Vis. 1997;10(4):433-6.</citation>
    <PMID>9176952</PMID>
  </reference>
  <reference>
    <citation>Cottaris NP, Jiang H, Ding X, Wandell BA, Brainard DH. A computational-observer model of spatial contrast sensitivity: Effects of wave-front-based optics, cone-mosaic structure, and inference engine. J Vis. 2019 Apr 1;19(4):8. doi: 10.1167/19.4.8.</citation>
    <PMID>30943530</PMID>
  </reference>
  <reference>
    <citation>Cottaris NP, Wandell BA, Rieke F, Brainard DH. A computational observer model of spatial contrast sensitivity: Effects of photocurrent encoding, fixational eye movements, and inference engine. J Vis. 2020 Jul 1;20(7):17. doi: 10.1167/jov.20.7.17.</citation>
    <PMID>32692826</PMID>
  </reference>
  <reference>
    <citation>DiCarlo JJ, Cox DD. Untangling invariant object recognition. Trends Cogn Sci. 2007 Aug;11(8):333-41. Epub 2007 Jul 16.</citation>
    <PMID>17631409</PMID>
  </reference>
  <reference>
    <citation>DiCarlo JJ, Zoccolan D, Rust NC. How does the brain solve visual object recognition? Neuron. 2012 Feb 9;73(3):415-34. doi: 10.1016/j.neuron.2012.01.010.</citation>
    <PMID>22325196</PMID>
  </reference>
  <reference>
    <citation>Gauthier I, Tarr MJ. Visual Object Recognition: Do We (Finally) Know More Now Than We Did? Annu Rev Vis Sci. 2016 Oct 14;2:377-396. doi: 10.1146/annurev-vision-111815-114621. Epub 2016 Aug 3. Review.</citation>
    <PMID>28532357</PMID>
  </reference>
  <reference>
    <citation>Heasly BS, Cottaris NP, Lichtman DP, Xiao B, Brainard DH. RenderToolbox3: MATLAB tools that facilitate physically based stimulus rendering for vision research. J Vis. 2014 Feb 7;14(2). pii: 6. doi: 10.1167/14.2.6.</citation>
    <PMID>24511145</PMID>
  </reference>
  <reference>
    <citation>Ni AM, Huang C, Doiron B, Cohen MR. A general decoding strategy explains the relationship between behavior and correlated variability. bioRxiv 2020.10.08.331850. doi: https://doi.org/10.1101/2020.10.08.331850.</citation>
  </reference>
  <reference>
    <citation>Ni AM, Ruff DA, Alberts JJ, Symmonds J, Cohen MR. Learning and attention reveal a general relationship between population activity and behavior. Science. 2018 Jan 26;359(6374):463-465. doi: 10.1126/science.aao0284.</citation>
    <PMID>29371470</PMID>
  </reference>
  <reference>
    <citation>Prins N, Kingdom FAA. Applying the Model-Comparison Approach to Test Specific Research Hypotheses in Psychophysical Research Using the Palamedes Toolbox. Front Psychol. 2018 Jul 23;9:1250. doi: 10.3389/fpsyg.2018.01250. eCollection 2018.</citation>
    <PMID>30083122</PMID>
  </reference>
  <reference>
    <citation>Ruff DA, Ni AM, Cohen MR. Cognition as a Window into Neuronal Population Space. Annu Rev Neurosci. 2018 Jul 8;41:77-97. doi: 10.1146/annurev-neuro-080317-061936. Review.</citation>
    <PMID>29799773</PMID>
  </reference>
  <reference>
    <citation>Singh V, Cottaris NP, Heasly BS, Brainard DH, Burge J. Computational luminance constancy from naturalistic images. J Vis. 2018 Dec 3;18(13):19. doi: 10.1167/18.13.19.</citation>
    <PMID>30593061</PMID>
  </reference>
  <reference>
    <citation>Singh V, Burge J, Brainard DH. Equivalent noise characterization of human lightness constancy. bioRxiv 2021.06.04.447171. doi: https://doi.org/10.1101/2021.06.04.447171.</citation>
  </reference>
  <verification_date>August 2021</verification_date>
  <study_first_submitted>August 6, 2021</study_first_submitted>
  <study_first_submitted_qc>August 6, 2021</study_first_submitted_qc>
  <study_first_posted type="Actual">August 13, 2021</study_first_posted>
  <last_update_submitted>August 12, 2021</last_update_submitted>
  <last_update_submitted_qc>August 12, 2021</last_update_submitted_qc>
  <last_update_posted type="Actual">August 19, 2021</last_update_posted>
  <responsible_party>
    <responsible_party_type>Sponsor</responsible_party_type>
  </responsible_party>
  <keyword>Perception</keyword>
  <keyword>Sensory Processing</keyword>
  <keyword>Vision</keyword>
  <provided_document_section>
    <provided_document>
      <document_type>Informed Consent Form</document_type>
      <document_has_protocol>No</document_has_protocol>
      <document_has_icf>Yes</document_has_icf>
      <document_has_sap>No</document_has_sap>
      <document_date>August 5, 2021</document_date>
      <document_url>https://ClinicalTrials.gov/ProvidedDocs/49/NCT05004649/ICF_001.pdf</document_url>
    </provided_document>
  </provided_document_section>
  <!-- Results have not yet been posted for this study                                          -->
</clinical_study>


<clinical_study>
  <!-- This xml conforms to an XML Schema at:
    https://clinicaltrials.gov/ct2/html/images/info/public.xsd -->
  <required_header>
    <download_date>ClinicalTrials.gov processed this data on November 19, 2021</download_date>
    <link_text>Link to the current ClinicalTrials.gov record.</link_text>
    <url>https://clinicaltrials.gov/show/NCT04858022</url>
  </required_header>
  <id_info>
    <org_study_id>20-21-2137-Study 2</org_study_id>
    <nct_id>NCT04858022</nct_id>
  </id_info>
  <brief_title>Visual Acoustic Biofeedback for RSE Via Telepractice</brief_title>
  <official_title>Online Assessment and Enhancement of Auditory Perception for Speech Sound Errors: Visual Acoustic Biofeedback for RSE Via Telepractice</official_title>
  <sponsors>
    <lead_sponsor>
      <agency>Montclair State University</agency>
      <agency_class>Other</agency_class>
    </lead_sponsor>
    <collaborator>
      <agency>National Institute on Deafness and Other Communication Disorders (NIDCD)</agency>
      <agency_class>NIH</agency_class>
    </collaborator>
    <collaborator>
      <agency>New York University</agency>
      <agency_class>Other</agency_class>
    </collaborator>
    <collaborator>
      <agency>Syracuse University</agency>
      <agency_class>Other</agency_class>
    </collaborator>
  </sponsors>
  <source>Montclair State University</source>
  <oversight_info>
    <has_dmc>Yes</has_dmc>
    <is_fda_regulated_drug>No</is_fda_regulated_drug>
    <is_fda_regulated_device>No</is_fda_regulated_device>
  </oversight_info>
  <brief_summary>
    <textblock>
      This research will meet a public health need by evaluating the efficacy of speech&#xD;
      intervention supplemented with real-time visual-acoustic biofeedback when delivered using&#xD;
      remote technologies.&#xD;
    </textblock>
  </brief_summary>
  <detailed_description>
    <textblock>
      In a crossover design, participants will be randomly assigned to receive 10 weeks/20 sessions&#xD;
      of visual-acoustic biofeedback treatment via telepractice followed by a 10-week period of no&#xD;
      treatment, or the same two conditions in the reverse order. Production accuracy will be&#xD;
      assessed with standard probes (20 syllables, 30 words, and 10 sentences containing /r/ in&#xD;
      various phonetic contexts) administered prior to the beginning of treatment (baseline), after&#xD;
      the first 10-week phase, and again at the end of the study.&#xD;
    </textblock>
  </detailed_description>
  <overall_status>Not yet recruiting</overall_status>
  <start_date type="Anticipated">September 15, 2021</start_date>
  <completion_date type="Anticipated">August 30, 2023</completion_date>
  <primary_completion_date type="Anticipated">August 30, 2023</primary_completion_date>
  <phase>Phase 2</phase>
  <study_type>Interventional</study_type>
  <has_expanded_access>No</has_expanded_access>
  <study_design_info>
    <allocation>Randomized</allocation>
    <intervention_model>Factorial Assignment</intervention_model>
    <intervention_model_description>Children with residual speech errors (RSE) and typical perception will assigned to a randomized controlled trial measuring the efficacy of online visual-acoustic biofeedback treatment relative to a waitlist comparison condition. Treatment will last a total of 20 weeks. The investigators will randomly assign 20 children to receive 10 weeks of biofeedback training via video call followed by 10 weeks in a no-treatment phase; 20 children will receive the same phases in reverse order. Children with RSE may vary in pre-treatment severity, and the extent to which they can approximate /r/ may be an important indicator of subsequent treatment response. Therefore, a blocked randomization procedure will be used to protect against a situation where treatment groups are unbalanced with respect to pre-treatment severity.</intervention_model_description>
    <primary_purpose>Treatment</primary_purpose>
    <masking>Single (Outcomes Assessor)</masking>
    <masking_description>All perceptual ratings will be obtained from blinded, skilled clinician listeners recruited through online crowdsourcing. Following protocols refined in previous published research, binary rating responses (1=correct; 0=incorrect) will be aggregated over at least 9 unique listeners per token.</masking_description>
  </study_design_info>
  <primary_outcome>
    <measure>Change in perceptually rated accuracy of /r/</measure>
    <time_frame>Pre-, mid- (after 10 weeks), and 1 week post-treatment</time_frame>
    <description>To assess generalization of treatment gains to untreated words, participants will be assessed with standard probes (30 words [considered the primary target], 20 syllables, and 10 sentences containing /r/ in various phonetic contexts). Stimuli in each probe will be presented individually in randomized order with blocking by stimulus type (word, syllable, sentence). No auditory models will be provided; for children with reading difficulty, semantic cues will be provided to elicit the intended word. Individual words will be isolated from the audio record of each word probe and presented in randomized order for binary rating (correct/incorrect) by 9 naïve listeners who are blind to treatment condition and time point (but will see the written representation of each target word). The investigators will use the proportion of &quot;correct&quot; ratings for each token as our primary measure of perceptually rated accuracy.</description>
  </primary_outcome>
  <secondary_outcome>
    <measure>Change in social-emotional well-being</measure>
    <time_frame>Pre- and 1 week post-treatment</time_frame>
    <description>This 11-item survey, which asks parents to report the impact of speech disorder on their child's social, emotional, and academic well-being, was validated in a published study by members of the research team. An impact score, calculated as described in our previous research, will be used as the primary measure of socio-emotional well-being.</description>
  </secondary_outcome>
  <number_of_arms>2</number_of_arms>
  <enrollment type="Anticipated">40</enrollment>
  <condition>Speech Sound Disorder</condition>
  <arm_group>
    <arm_group_label>Visual Acoustic Biofeedback for RSE via Telepractice-Treatment</arm_group_label>
    <arm_group_type>Experimental</arm_group_type>
    <description>Condition1: Treatment-first&#xD;
Children with RSE and typical perception will be allocated to a randomized controlled trial measuring the efficacy of online visual-acoustic biofeedback treatment. Ten children with RSE will receive 10 weeks of visual-acoustic biofeedback training via video call.</description>
  </arm_group>
  <arm_group>
    <arm_group_label>Visual Acoustic Biofeedback for RSE via Telepractice-Wait</arm_group_label>
    <arm_group_type>Experimental</arm_group_type>
    <description>Condition 2: Waitlist-first&#xD;
Following the initial evaluation, 10 children with RSE will be allocated to a 10 week no treatment condition.</description>
  </arm_group>
  <intervention>
    <intervention_type>Behavioral</intervention_type>
    <intervention_name>Visual Acoustic Biofeedback</intervention_name>
    <description>In visual-acoustic biofeedback treatment, the elements of traditional articulatory treatment (i.e., auditory models and verbal descriptions of articulator placement) are enhanced with a dynamic display of the speech signal in the form of the real-time LPC (Linear Predictive Coding) spectrum. Because correct vs incorrect productions of /r/ contrast acoustically in the frequency of the third formant (F3), participants will be cued to make their real-time LPC spectrum match a visual target characterized by a low F3 frequency. They will be encouraged to attend to the visual display while adjusting the placement of their articulators and observing how those adjustments impact F3.</description>
    <arm_group_label>Visual Acoustic Biofeedback for RSE via Telepractice-Treatment</arm_group_label>
  </intervention>
  <intervention>
    <intervention_type>Other</intervention_type>
    <intervention_name>No treatment - waitlist</intervention_name>
    <description>10-week period of no treatment</description>
    <arm_group_label>Visual Acoustic Biofeedback for RSE via Telepractice-Wait</arm_group_label>
  </intervention>
  <eligibility>
    <criteria>
      <textblock>
        Inclusion Criteria:&#xD;
&#xD;
          -  Must be between 9;0 and 15;11 years of age at the time of enrollment.&#xD;
&#xD;
          -  Must speak English as the dominant language (i.e., must have begun learning English by&#xD;
             age 2, per parent report).&#xD;
&#xD;
          -  Must speak a rhotic dialect of English.&#xD;
&#xD;
          -  Must pass a pure-tone hearing screening at 20dB hearing level.&#xD;
&#xD;
          -  Must pass a brief examination of oral structure and function.&#xD;
&#xD;
          -  Must exhibit less than 30% accuracy, based on consensus across 2 trained listeners, on&#xD;
             a probe list eliciting rhotics in various phonetic contexts at the word level.&#xD;
&#xD;
          -  Must exhibit no more than 3 sounds other than /r/ in error on the Goldman-Fristoe Test&#xD;
             of Articulation-3 (GFTA-3).&#xD;
&#xD;
        Exclusion Criteria:&#xD;
&#xD;
          -  Must not receive a T score more than 1.3 SD below the mean on the Wechsler Abbreviated&#xD;
             Scale of Intelligence-2 (WASI-2) Matrix Reasoning&#xD;
&#xD;
          -  Must not receive a scaled score of 7 or higher on the Recalling Sentences and&#xD;
             Formulated Sentences subtests of the Clinical Evaluation of Language Fundamentals-5&#xD;
             (CELF-5).&#xD;
&#xD;
          -  Must not have an existing diagnosis of developmental disability or major&#xD;
             neurobehavioral syndrome such as cerebral palsy, Down Syndrome, or Autism Spectrum&#xD;
             Disorder&#xD;
      </textblock>
    </criteria>
    <gender>All</gender>
    <minimum_age>9 Years</minimum_age>
    <maximum_age>15 Years</maximum_age>
    <healthy_volunteers>Accepts Healthy Volunteers</healthy_volunteers>
  </eligibility>
  <overall_contact>
    <last_name>Elaine R Hitchcock, PhD</last_name>
    <phone>973-229-3797</phone>
    <email>hitchcocke@montclair.edu</email>
  </overall_contact>
  <reference>
    <citation>Preston JL, Hitchcock ER, Leece MC. Auditory Perception and Ultrasound Biofeedback Treatment Outcomes for Children With Residual /ɹ/ Distortions: A Randomized Controlled Trial. J Speech Lang Hear Res. 2020 Feb 26;63(2):444-455. doi: 10.1044/2019_JSLHR-19-00060. Epub 2020 Feb 26.</citation>
    <PMID>32097058</PMID>
  </reference>
  <reference>
    <citation>Byun TM, Hitchcock ER. Investigating the use of traditional and spectral biofeedback approaches to intervention for /r/ misarticulation. Am J Speech Lang Pathol. 2012 Aug;21(3):207-21. doi: 10.1044/1058-0360(2012/11-0083). Epub 2012 Mar 21.</citation>
    <PMID>22442281</PMID>
  </reference>
  <reference>
    <citation>McAllister Byun T. Efficacy of Visual-Acoustic Biofeedback Intervention for Residual Rhotic Errors: A Single-Subject Randomization Study. J Speech Lang Hear Res. 2017 May 24;60(5):1175-1193. doi: 10.1044/2016_JSLHR-S-16-0038.</citation>
    <PMID>28389677</PMID>
  </reference>
  <reference>
    <citation>Byun TM, Campbell H, Carey H, Liang W, Park TH, Svirsky M. Enhancing Intervention for Residual Rhotic Errors Via App-Delivered Biofeedback: A Case Study. J Speech Lang Hear Res. 2017 Jun 22;60(6S):1810-1817. doi: 10.1044/2017_JSLHR-S-16-0248.</citation>
    <PMID>28655050</PMID>
  </reference>
  <reference>
    <citation>McAllister Byun T, Harel D, Halpin PF, Szeredi D. Deriving gradient measures of child speech from crowdsourced ratings. J Commun Disord. 2016 Nov - Dec;64:91-102. doi: 10.1016/j.jcomdis.2016.07.001. Epub 2016 Jul 6.</citation>
    <PMID>27481555</PMID>
  </reference>
  <reference>
    <citation>Harel D, Hitchcock ER, Szeredi D, Ortiz J, McAllister Byun T. Finding the experts in the crowd: Validity and reliability of crowdsourced measures of children's gradient speech contrasts. Clin Linguist Phon. 2017;31(1):104-117. Epub 2016 Jun 7.</citation>
    <PMID>27267258</PMID>
  </reference>
  <reference>
    <citation>Preston JL, Benway NR, Leece MC, Hitchcock ER, McAllister T. Tutorial: Motor-Based Treatment Strategies for /r/ Distortions. Lang Speech Hear Serv Sch. 2020 Oct 2;51(4):966-980. doi: 10.1044/2020_LSHSS-20-00012. Epub 2020 Aug 12. Review.</citation>
    <PMID>32783706</PMID>
  </reference>
  <reference>
    <citation>Byun TM, Hitchcock ER, Swartz MT. Retroflex versus bunched in treatment for rhotic misarticulation: evidence from ultrasound biofeedback intervention. J Speech Lang Hear Res. 2014 Dec;57(6):2116-30. doi: 10.1044/2014_JSLHR-S-14-0034.</citation>
    <PMID>25088034</PMID>
  </reference>
  <reference>
    <citation>Hitchcock, ER, Cabbage, KL, T. Swartz, M, Carrell, TD. Measuring Speech Perception Using the Wide-Range Acoustic Accuracy Scale: Preliminary Findings. Perspectives of the ASHA Special Interest Groups, 5(4):1098-1112, 2020.</citation>
  </reference>
  <reference>
    <citation>Dugan SH, Silbert N, McAllister T, Preston JL, Sotto C, Boyce SE. Modelling category goodness judgments in children with residual sound errors. Clin Linguist Phon. 2019;33(4):295-315. doi: 10.1080/02699206.2018.1477834. Epub 2018 May 24.</citation>
    <PMID>29792525</PMID>
  </reference>
  <reference>
    <citation>McAllister Byun T, Tiede M. Perception-production relations in later development of American English rhotics. PLoS One. 2017 Feb 16;12(2):e0172022. doi: 10.1371/journal.pone.0172022. eCollection 2017.</citation>
    <PMID>28207800</PMID>
  </reference>
  <reference>
    <citation>McAllister T, Preston JL, Hitchcock ER, Hill J. Protocol for Correcting Residual Errors with Spectral, ULtrasound, Traditional Speech therapy Randomized Controlled Trial (C-RESULTS RCT). BMC Pediatr. 2020 Feb 11;20(1):66. doi: 10.1186/s12887-020-1941-5.</citation>
    <PMID>32046671</PMID>
  </reference>
  <reference>
    <citation>Hitchcock ER, Byun TM. Enhancing generalisation in biofeedback intervention using the challenge point framework: a case study. Clin Linguist Phon. 2015 Jan;29(1):59-75. doi: 10.3109/02699206.2014.956232. Epub 2014 Sep 12.</citation>
    <PMID>25216375</PMID>
  </reference>
  <reference>
    <citation>McAllister Byun T, Swartz MT, Halpin PF, Szeredi D, Maas E. Direction of attentional focus in biofeedback treatment for /r/ misarticulation. Int J Lang Commun Disord. 2016 Jul;51(4):384-401. doi: 10.1111/1460-6984.12215. Epub 2016 Mar 6.</citation>
    <PMID>26947142</PMID>
  </reference>
  <reference>
    <citation>Byun TM, Hitchcock ER, Ferron J. Masked Visual Analysis: Minimizing Type I Error in Visually Guided Single-Case Design for Communication Disorders. J Speech Lang Hear Res. 2017 Jun 10;60(6):1455-1466. doi: 10.1044/2017_JSLHR-S-16-0344.</citation>
    <PMID>28595354</PMID>
  </reference>
  <reference>
    <citation>Hitchcock ER, Swartz MT, Lopez M. Speech Sound Disorder and Visual Biofeedback Intervention: A Preliminary Investigation of Treatment Intensity. Semin Speech Lang. 2019 Mar;40(2):124-137. doi: 10.1055/s-0039-1677763. Epub 2019 Feb 22.</citation>
    <PMID>30795023</PMID>
  </reference>
  <reference>
    <citation>Hitchcock ER, Harel D, Byun TM. Social, Emotional, and Academic Impact of Residual Speech Errors in School-Aged Children: A Survey Study. Semin Speech Lang. 2015 Nov;36(4):283-94. doi: 10.1055/s-0035-1562911. Epub 2015 Oct 12.</citation>
    <PMID>26458203</PMID>
  </reference>
  <reference>
    <citation>Hitchcock ER, Byun TM, Swartz M, Lazarus R. Efficacy of Electropalatography for Treating Misarticulation of /r/. Am J Speech Lang Pathol. 2017 Nov 8;26(4):1141-1158. doi: 10.1044/2017_AJSLP-16-0122.</citation>
    <PMID>28834534</PMID>
  </reference>
  <reference>
    <citation>Hitchcock ER, Koenig LL. Longitudinal observations of typical English voicing acquisition in a 2-year-old child: Stability of the contrast and considerations for clinical assessment. Clin Linguist Phon. 2015;29(12):955-76. doi: 10.3109/02699206.2015.1083617. Epub 2015 Oct 29.</citation>
    <PMID>26513374</PMID>
  </reference>
  <verification_date>August 2021</verification_date>
  <study_first_submitted>April 5, 2021</study_first_submitted>
  <study_first_submitted_qc>April 22, 2021</study_first_submitted_qc>
  <study_first_posted type="Actual">April 23, 2021</study_first_posted>
  <last_update_submitted>August 30, 2021</last_update_submitted>
  <last_update_submitted_qc>August 30, 2021</last_update_submitted_qc>
  <last_update_posted type="Actual">August 31, 2021</last_update_posted>
  <responsible_party>
    <responsible_party_type>Principal Investigator</responsible_party_type>
    <investigator_affiliation>Montclair State University</investigator_affiliation>
    <investigator_full_name>Elaine Hitchcock</investigator_full_name>
    <investigator_title>Associate Professor</investigator_title>
  </responsible_party>
  <keyword>speech</keyword>
  <keyword>articulation</keyword>
  <keyword>auditory perception</keyword>
  <condition_browse>
    <!-- CAUTION:  The following MeSH terms are assigned with an imperfect algorithm            -->
    <mesh_term>Speech Sound Disorder</mesh_term>
  </condition_browse>
  <patient_data>
    <sharing_ipd>No</sharing_ipd>
  </patient_data>
  <!-- Results have not yet been posted for this study                                          -->
</clinical_study>


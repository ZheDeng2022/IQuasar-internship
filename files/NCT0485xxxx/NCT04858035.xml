<clinical_study>
  <!-- This xml conforms to an XML Schema at:
    https://clinicaltrials.gov/ct2/html/images/info/public.xsd -->
  <required_header>
    <download_date>ClinicalTrials.gov processed this data on November 19, 2021</download_date>
    <link_text>Link to the current ClinicalTrials.gov record.</link_text>
    <url>https://clinicaltrials.gov/show/NCT04858035</url>
  </required_header>
  <id_info>
    <org_study_id>20-21-2137-Study 3</org_study_id>
    <nct_id>NCT04858035</nct_id>
  </id_info>
  <brief_title>Auditory-Perceptual Training Via Telepractice</brief_title>
  <official_title>Online Assessment and Enhancement of Auditory Perception for Speech Sound Errors: Online Perceptual Training for RSE</official_title>
  <sponsors>
    <lead_sponsor>
      <agency>Montclair State University</agency>
      <agency_class>Other</agency_class>
    </lead_sponsor>
    <collaborator>
      <agency>National Institute on Deafness and Other Communication Disorders (NIDCD)</agency>
      <agency_class>NIH</agency_class>
    </collaborator>
    <collaborator>
      <agency>Syracuse University</agency>
      <agency_class>Other</agency_class>
    </collaborator>
    <collaborator>
      <agency>New York University</agency>
      <agency_class>Other</agency_class>
    </collaborator>
  </sponsors>
  <source>Montclair State University</source>
  <oversight_info>
    <has_dmc>Yes</has_dmc>
    <is_fda_regulated_drug>No</is_fda_regulated_drug>
    <is_fda_regulated_device>No</is_fda_regulated_device>
  </oversight_info>
  <brief_summary>
    <textblock>
      The objective of this study is to measure the effects of online perceptual training on&#xD;
      perception and production in children with RSE who exhibit atypical perception relative to&#xD;
      norms from our lab-based pilot data. In a multiple-baseline across-subjects design, 10&#xD;
      children with RSE will begin in a baseline phase probing perceptual acuity for /r/.&#xD;
      Perceptual training with multiple types of stimuli will be initiated in a staggered fashion.&#xD;
      Production probes elicited before and after treatment will assess the extent to which&#xD;
      perception gains transfer to /r/ production.&#xD;
    </textblock>
  </brief_summary>
  <detailed_description>
    <textblock>
      Following the initial evaluation to determine eligibility, participants will be enrolled in a&#xD;
      baseline phase in which perception and production abilities will be probed but not treated.&#xD;
      In a multiple-baseline across-subjects design with randomization, the 10 participants will be&#xD;
      randomly assigned to transition from the baseline to the treatment condition at one of 7&#xD;
      possible points, ranging from 4 to 10 baseline sessions. After baseline sessions, perceptual&#xD;
      training will be delivered in twelve 30-minute sessions occurring 3 times per week for 4&#xD;
      weeks. The training is fully computerized and can be self-administered, but a study clinician&#xD;
      will attend one session per participant per week to ensure attention to study tasks. Each&#xD;
      session will feature three listening tasks of approximately equal duration. Following&#xD;
      perceptual training, participants will receive 4 60-minute sessions of production training&#xD;
      completed over two weeks.&#xD;
    </textblock>
  </detailed_description>
  <overall_status>Not yet recruiting</overall_status>
  <start_date type="Anticipated">August 1, 2021</start_date>
  <completion_date type="Anticipated">August 30, 2024</completion_date>
  <primary_completion_date type="Anticipated">August 30, 2023</primary_completion_date>
  <phase>Phase 1</phase>
  <study_type>Interventional</study_type>
  <has_expanded_access>No</has_expanded_access>
  <study_design_info>
    <allocation>N/A</allocation>
    <intervention_model>Single Group Assignment</intervention_model>
    <intervention_model_description>This study follows a multiple-baseline across-subjects design with random assignment of participants to different baseline durations. Following the initial evaluation to determine eligibility, participants will be enrolled in a baseline phase in which perception and production abilities will be probed but not treated. Perception will be measured using the online identification and category goodness tasks and production will be measured from standard probes (20 syllables, 30 words, and 10 sentences containing /r/ in various phonetic contexts). Participants will be randomly assigned to transition from the baseline to the treatment condition at one of 7 possible points, ranging from 4 to 10 baseline sessions. All participants will receive 12 sessions of perceptual training over 4 weeks followed by 4 sessions of production training over 2 weeks.</intervention_model_description>
    <primary_purpose>Treatment</primary_purpose>
    <masking>None (Open Label)</masking>
  </study_design_info>
  <primary_outcome>
    <measure>Change in percent accuracy pooled across perception tasks</measure>
    <time_frame>Pre- and within-treatment (up to one month); pre and 1 week post-treatment</time_frame>
    <description>Treatment tasks 2 (identification) and 3 (category goodness judgment) are identical to the tasks used to assess performance in the baseline phase, with the single exception that accuracy feedback is provided during the treatment phase.&#xD;
Participants' progress will be tracked continuously across baseline and treatment phases using percent accuracy on these two tasks. In Task 2, accuracy in classifying stimuli as /r/ or /w/ will be assessed relative to the mean across responses from typical participants in the research team's lab-based pilot data. In Task 3, accuracy in category goodness judgment will be assessed relative to the &quot;goldstandard&quot; ratings determined by consensus across at least four expert listeners.</description>
  </primary_outcome>
  <secondary_outcome>
    <measure>Change in percent accuracy per perception task</measure>
    <time_frame>Pre- and within-treatment (up to one month); pre- and 1 week post-treatment</time_frame>
    <description>Consistent with the primary analysis, participants' progress will be tracked continuously across baseline and treatment phases using percent accuracy; however, percent accuracy scores will be analyzed separately for comparison between tasks (category goodness judgment and identification tasks) using the same analyses as the primary outcome.</description>
  </secondary_outcome>
  <secondary_outcome>
    <measure>Perceptually rated accuracy of /r/ production</measure>
    <time_frame>Pre- and 1 week post-treatment</time_frame>
    <description>To assess generalization of perceptual training to production, participants will be assessed with standard probes administered in the first three baseline sessions and in three post-treatment maintenance sessions. Probes will elicit 30 words [considered the primary target], 20 syllables, and 10 sentences containing /r/ in various phonetic contexts. Stimuli in each probe will be presented individually in randomized order. No auditory models will be provided; for children with reading difficulty, semantic cues will be provided to elicit the intended word. Individual words will be isolated from the audio record of each word probe and presented in randomized order for binary rating (1=correct; 0=incorrect) by 9 naive listeners who are blind to treatment condition and time point (but will see the written representation of each target word). The research team will use the proportion of &quot;correct&quot; ratings for each token as our measure of perceptually rated accuracy.</description>
  </secondary_outcome>
  <number_of_arms>1</number_of_arms>
  <enrollment type="Anticipated">10</enrollment>
  <condition>Speech Sound Disorder</condition>
  <arm_group>
    <arm_group_label>Online Speech Training</arm_group_label>
    <arm_group_type>Experimental</arm_group_type>
    <description>Participants will be randomly assigned to transition from the baseline to the treatment condition at one of 7 possible points, ranging from 4 to 10 baseline sessions. All participants will receive 12 sessions of perceptual training over 4 weeks. Finally, participants will complete a 3-session maintenance phase in which perception and production are probed but not treated. Perception will be measured using the identification and category goodness judgment.&#xD;
Following completion of perception training all participants will complete two weeks of production training. The production training will consists of 4, 60-minute sessions. Each session will provide instruction and practice trials.</description>
  </arm_group>
  <intervention>
    <intervention_type>Behavioral</intervention_type>
    <intervention_name>Perceptual training</intervention_name>
    <description>Perceptual training involves self-paced presentation of auditory stimuli via a computerized software program.&#xD;
Stimuli are organized into three separate tasks:&#xD;
Tasks 1 and 3 will train category goodness judgment: Participants will hear 75 naturally produced speech tokens containing /r/ from various speakers, with a balance of correct and incorrect productions. They will classify each /r/ as correct or incorrect and receive feedback on the accuracy of their classification. Tasks 1 and 3 differ in that task 1 will feature a subset of items designed to provide focused practice on a specific context (e.g., initial /r/ as in red; /r/ as syllable nucleus as in sir), with increasing difficulty over time, whereas task 3 will feature randomly selected items representing all contexts and difficulty levels.&#xD;
Task 2: Participants will hear 75 items drawn from the synthetic rake-wake continuum used in the identification task administered at baseline.</description>
    <arm_group_label>Online Speech Training</arm_group_label>
  </intervention>
  <intervention>
    <intervention_type>Behavioral</intervention_type>
    <intervention_name>Production training</intervention_name>
    <description>Instruction: Discuss tongue shapes for /r/. Pre-practice: relatively unstructured, highly interactive elicitation; provide models and placement cue and KP on every trial.&#xD;
Structured syllable practice: Practice will occur in blocks of 10 consecutive trials on the same syllable (e.g., 10 /ra/), after which a new syllable will be addressed (e.g., 10 /re/--but note that in the fully blocked condition, the same syllable should occur in two consecutive blocks of 10).</description>
    <arm_group_label>Online Speech Training</arm_group_label>
  </intervention>
  <eligibility>
    <criteria>
      <textblock>
        Inclusion Criteria:&#xD;
&#xD;
          -  Must be between 9;0 and 15;11 years of age at the time of enrollment.&#xD;
&#xD;
          -  Must speak English as the dominant language (i.e., must have begun learning English by&#xD;
             age 2, per parent report).&#xD;
&#xD;
          -  Must speak a rhotic dialect of English.&#xD;
&#xD;
          -  Must pass a pure-tone hearing screening at 20dB HL&#xD;
&#xD;
          -  Must pass a brief examination of oral structure and function.&#xD;
&#xD;
          -  Must exhibit less than 30% accuracy, based on consensus across 2 trained listeners, on&#xD;
             a probe list eliciting rhotics in various phonetic contexts at the word level.&#xD;
&#xD;
          -  Must exhibit no more than 3 sounds other than /r/ in error on the GFTA-3&#xD;
&#xD;
        Exclusion Criteria:&#xD;
&#xD;
          -  Must not receive a T score more than 1.3 SD below the mean on the Wechsler Abbreviated&#xD;
             Scale of Intelligence-2 (WASI-2) Matrix Reasoning&#xD;
&#xD;
          -  Must not receive a scaled score of 7 or higher on the Recalling Sentences and&#xD;
             Formulated Sentences subtests of the Clinical Evaluation of Language Fundamentals-5&#xD;
             (CELF-5).&#xD;
&#xD;
          -  Must not have an existing diagnosis of developmental disability or major&#xD;
             neurobehavioral syndrome such as cerebral palsy, Down Syndrome, or Autism Spectrum&#xD;
             Disorder&#xD;
      </textblock>
    </criteria>
    <gender>All</gender>
    <minimum_age>9 Years</minimum_age>
    <maximum_age>15 Years</maximum_age>
    <healthy_volunteers>Accepts Healthy Volunteers</healthy_volunteers>
  </eligibility>
  <overall_contact>
    <last_name>Elaine Hitchcock, PhD</last_name>
    <phone>973-229-3797</phone>
    <email>hitchcocke@montclair.edu</email>
  </overall_contact>
  <reference>
    <citation>Preston JL, Hitchcock ER, Leece MC. Auditory Perception and Ultrasound Biofeedback Treatment Outcomes for Children With Residual /É¹/ Distortions: A Randomized Controlled Trial. J Speech Lang Hear Res. 2020 Feb 26;63(2):444-455. doi: 10.1044/2019_JSLHR-19-00060. Epub 2020 Feb 26.</citation>
    <PMID>32097058</PMID>
  </reference>
  <reference>
    <citation>Byun TM, Hitchcock ER. Investigating the use of traditional and spectral biofeedback approaches to intervention for /r/ misarticulation. Am J Speech Lang Pathol. 2012 Aug;21(3):207-21. doi: 10.1044/1058-0360(2012/11-0083). Epub 2012 Mar 21.</citation>
    <PMID>22442281</PMID>
  </reference>
  <reference>
    <citation>McAllister Byun T. Efficacy of Visual-Acoustic Biofeedback Intervention for Residual Rhotic Errors: A Single-Subject Randomization Study. J Speech Lang Hear Res. 2017 May 24;60(5):1175-1193. doi: 10.1044/2016_JSLHR-S-16-0038.</citation>
    <PMID>28389677</PMID>
  </reference>
  <reference>
    <citation>McAllister Byun T, Harel D, Halpin PF, Szeredi D. Deriving gradient measures of child speech from crowdsourced ratings. J Commun Disord. 2016 Nov - Dec;64:91-102. doi: 10.1016/j.jcomdis.2016.07.001. Epub 2016 Jul 6.</citation>
    <PMID>27481555</PMID>
  </reference>
  <reference>
    <citation>Harel D, Hitchcock ER, Szeredi D, Ortiz J, McAllister Byun T. Finding the experts in the crowd: Validity and reliability of crowdsourced measures of children's gradient speech contrasts. Clin Linguist Phon. 2017;31(1):104-117. Epub 2016 Jun 7.</citation>
    <PMID>27267258</PMID>
  </reference>
  <reference>
    <citation>Preston JL, Benway NR, Leece MC, Hitchcock ER, McAllister T. Tutorial: Motor-Based Treatment Strategies for /r/ Distortions. Lang Speech Hear Serv Sch. 2020 Oct 2;51(4):966-980. doi: 10.1044/2020_LSHSS-20-00012. Epub 2020 Aug 12. Review.</citation>
    <PMID>32783706</PMID>
  </reference>
  <reference>
    <citation>Byun TM, Hitchcock ER, Swartz MT. Retroflex versus bunched in treatment for rhotic misarticulation: evidence from ultrasound biofeedback intervention. J Speech Lang Hear Res. 2014 Dec;57(6):2116-30. doi: 10.1044/2014_JSLHR-S-14-0034.</citation>
    <PMID>25088034</PMID>
  </reference>
  <reference>
    <citation>Dugan SH, Silbert N, McAllister T, Preston JL, Sotto C, Boyce SE. Modelling category goodness judgments in children with residual sound errors. Clin Linguist Phon. 2019;33(4):295-315. doi: 10.1080/02699206.2018.1477834. Epub 2018 May 24.</citation>
    <PMID>29792525</PMID>
  </reference>
  <reference>
    <citation>McAllister Byun T, Tiede M. Perception-production relations in later development of American English rhotics. PLoS One. 2017 Feb 16;12(2):e0172022. doi: 10.1371/journal.pone.0172022. eCollection 2017.</citation>
    <PMID>28207800</PMID>
  </reference>
  <reference>
    <citation>McAllister T, Preston JL, Hitchcock ER, Hill J. Protocol for Correcting Residual Errors with Spectral, ULtrasound, Traditional Speech therapy Randomized Controlled Trial (C-RESULTS RCT). BMC Pediatr. 2020 Feb 11;20(1):66. doi: 10.1186/s12887-020-1941-5.</citation>
    <PMID>32046671</PMID>
  </reference>
  <reference>
    <citation>Hitchcock ER, Byun TM. Enhancing generalisation in biofeedback intervention using the challenge point framework: a case study. Clin Linguist Phon. 2015 Jan;29(1):59-75. doi: 10.3109/02699206.2014.956232. Epub 2014 Sep 12.</citation>
    <PMID>25216375</PMID>
  </reference>
  <reference>
    <citation>McAllister Byun T, Swartz MT, Halpin PF, Szeredi D, Maas E. Direction of attentional focus in biofeedback treatment for /r/ misarticulation. Int J Lang Commun Disord. 2016 Jul;51(4):384-401. doi: 10.1111/1460-6984.12215. Epub 2016 Mar 6.</citation>
    <PMID>26947142</PMID>
  </reference>
  <reference>
    <citation>Hitchcock, ER, Cabbage, KL, Swartz, M, Carrell, T. Measuring Speech Perception Using the Wide-Range Acoustic Accuracy Scale: Preliminary Findings. Perspectives of the ASHA Special Interest Groups, 5(4):1098-1112, 2020.</citation>
  </reference>
  <reference>
    <citation>Hitchcock ER, Swartz MT, Lopez M. Speech Sound Disorder and Visual Biofeedback Intervention: A Preliminary Investigation of Treatment Intensity. Semin Speech Lang. 2019 Mar;40(2):124-137. doi: 10.1055/s-0039-1677763. Epub 2019 Feb 22.</citation>
    <PMID>30795023</PMID>
  </reference>
  <reference>
    <citation>Byun TM, Hitchcock ER, Ferron J. Masked Visual Analysis: Minimizing Type I Error in Visually Guided Single-Case Design for Communication Disorders. J Speech Lang Hear Res. 2017 Jun 10;60(6):1455-1466. doi: 10.1044/2017_JSLHR-S-16-0344.</citation>
    <PMID>28595354</PMID>
  </reference>
  <reference>
    <citation>Hitchcock ER, Harel D, Byun TM. Social, Emotional, and Academic Impact of Residual Speech Errors in School-Aged Children: A Survey Study. Semin Speech Lang. 2015 Nov;36(4):283-94. doi: 10.1055/s-0035-1562911. Epub 2015 Oct 12.</citation>
    <PMID>26458203</PMID>
  </reference>
  <verification_date>April 2021</verification_date>
  <study_first_submitted>April 5, 2021</study_first_submitted>
  <study_first_submitted_qc>April 22, 2021</study_first_submitted_qc>
  <study_first_posted type="Actual">April 26, 2021</study_first_posted>
  <last_update_submitted>April 23, 2021</last_update_submitted>
  <last_update_submitted_qc>April 23, 2021</last_update_submitted_qc>
  <last_update_posted type="Actual">April 26, 2021</last_update_posted>
  <responsible_party>
    <responsible_party_type>Principal Investigator</responsible_party_type>
    <investigator_affiliation>Montclair State University</investigator_affiliation>
    <investigator_full_name>Elaine Hitchcock</investigator_full_name>
    <investigator_title>Associate Professor</investigator_title>
  </responsible_party>
  <keyword>speech sound disorder</keyword>
  <keyword>auditory perception</keyword>
  <keyword>articulation</keyword>
  <condition_browse>
    <!-- CAUTION:  The following MeSH terms are assigned with an imperfect algorithm            -->
    <mesh_term>Speech Sound Disorder</mesh_term>
  </condition_browse>
  <!-- Results have not yet been posted for this study                                          -->
</clinical_study>


<clinical_study>
  <!-- This xml conforms to an XML Schema at:
    https://clinicaltrials.gov/ct2/html/images/info/public.xsd -->
  <required_header>
    <download_date>ClinicalTrials.gov processed this data on November 19, 2021</download_date>
    <link_text>Link to the current ClinicalTrials.gov record.</link_text>
    <url>https://clinicaltrials.gov/show/NCT03764761</url>
  </required_header>
  <id_info>
    <org_study_id>Storybook</org_study_id>
    <secondary_id>R01HD083381-01A1</secondary_id>
    <nct_id>NCT03764761</nct_id>
  </id_info>
  <brief_title>Storybook Reading in Individuals With Down Syndrome</brief_title>
  <official_title>Eye Tracking Technologies to Characterize and Optimize Visual Attending in Down Syndrome</official_title>
  <sponsors>
    <lead_sponsor>
      <agency>Penn State University</agency>
      <agency_class>Other</agency_class>
    </lead_sponsor>
    <collaborator>
      <agency>Eunice Kennedy Shriver National Institute of Child Health and Human Development (NICHD)</agency>
      <agency_class>NIH</agency_class>
    </collaborator>
  </sponsors>
  <source>Penn State University</source>
  <oversight_info>
    <is_fda_regulated_drug>No</is_fda_regulated_drug>
    <is_fda_regulated_device>Yes</is_fda_regulated_device>
    <is_us_export>No</is_us_export>
  </oversight_info>
  <brief_summary>
    <textblock>
      This study uses mobile eye-tracking technology in order to characterize patterns of visual&#xD;
      attention to communication supports, as well as a partner, within real world interactions for&#xD;
      individuals with Down syndrome.&#xD;
&#xD;
      Visual communication supports are central components of what is termed augmentative and&#xD;
      alternative communication (AAC) intervention. AAC refers to the methods and technology&#xD;
      designed to supplement spoken communication for people with limited speech. &quot;Aided&quot; AAC is a&#xD;
      subcategory in which an external aid stores and presents for use visual symbols such as&#xD;
      photographs, line drawings, or alphabet letters. The most traditional means of structuring&#xD;
      aided AAC displays is to present the language concepts within row-column grids, which contain&#xD;
      individual symbols/concepts placed in each grid square. The investigator's previous work&#xD;
      investigated whether these grid-based presentations could be improved by understanding how&#xD;
      different perceptual features of the displays influence responding (i.e., whether what the&#xD;
      display looks like influences how easily the information on it is found). Individuals with&#xD;
      developmental disabilities and children developing typically were faster and more accurate in&#xD;
      finding information on some displays over others, when tested using a &quot;visual search&quot; task&#xD;
      (aka, a &quot;finding game&quot; - &quot;find the dog&quot;).&#xD;
&#xD;
      The previous investigations have evaluated visual attention within a setting that isolated&#xD;
      visual processing of the AAC display as the primary dependent measure. However, communication&#xD;
      requires attention not only to an AAC display, but also to a communication partner.&#xD;
      Therefore, the current study seeks to examine questions of visual attention to both an AAC&#xD;
      display and a communication partner. The investigators will manipulate characteristics of the&#xD;
      structure of the display (e.g., arrangement of symbols), in order to determine if more&#xD;
      optimal displays facilitate desirable patterns of visual attention to both the communication&#xD;
      display and the partner. The mobile eye-tracking technology captures attention to both the&#xD;
      display and the communication partner. The investigators anticipate that participants will be&#xD;
      able to attend to their partner and the shared activity more when the AAC display is more&#xD;
      optimal, but that when the AAC display is sub-optimal, the participants will have to spend&#xD;
      more time examining the AAC display and less time in actual communication.&#xD;
    </textblock>
  </brief_summary>
  <detailed_description>
    <textblock>
      Visual supports are central components of what is termed augmentative and alternative&#xD;
      communication (AAC) intervention within speech-language pathology. AAC refers to the methods&#xD;
      and technology designed to supplement spoken communication for people with limited speech.&#xD;
      &quot;Aided&quot; AAC is a subcategory in which an external aid stores and presents for use visual&#xD;
      symbols such as photographs, line drawings, or alphabet letters. Aided AAC relies on vision&#xD;
      for access. If users cannot fully attend to, understand, or process the semantic information&#xD;
      on a visual display, they are unlikely to use that display effectively. Regrettably, little&#xD;
      research has focused on AAC display design variables that enhance attention&#xD;
&#xD;
      This research seeks to gain a greater understanding of visual attention to AAC displays and&#xD;
      communication partners in order to further optimize display design. Eye tracking technology&#xD;
      will reveal attention patterns that typically go unrecorded in behavioral research,&#xD;
      particularly in individuals with severe disabilities. Specifically, eye tracking technology&#xD;
      permits recording of the coordinates of where the participant is looking at any given time,&#xD;
      how long they look, and what they ignore. This study seeks to record eye gaze via eye&#xD;
      tracking during a shared book reading activity in which the AAC display is used for&#xD;
      communication with a partner. It will help to determine whether optimal displays, which&#xD;
      facilitate speed to locate targets and minimize fixations to distractors, will promote&#xD;
      attention to the partner. Ultimately, this information will contribute to improving the&#xD;
      design of materials for children with disabilities who require AAC.&#xD;
&#xD;
      The most traditional means of structuring aided AAC displays is to present the language&#xD;
      concepts within traditional row-column grids, which contain individual symbols/concepts&#xD;
      placed in each grid square. The investigator's earlier work examined whether these grid-based&#xD;
      presentations could be improved by understanding how different perceptual features of the&#xD;
      displays influence responding (ie, whether what the display looks like influences how easily&#xD;
      the information on it is found). Individuals with developmental disabilities and children&#xD;
      developing typically were faster and more accurate in finding information on some displays&#xD;
      over others, when tested using a &quot;visual search&quot; task (aka, a &quot;finding game&quot; - &quot;find the&#xD;
      dog&quot;). The next study then examined the reason behind this phenomenon by using eye tracking&#xD;
      technology to examine how visual search itself was influenced by the different displays.&#xD;
      Results indicated that in individuals with and without disabilities, in the non-optimal&#xD;
      display there were significantly more fixations (looks) to non-relevant distractors than on&#xD;
      the optimal display. Given that individuals with disabilities, including Down syndrome, are&#xD;
      prone to ready distraction, the use of a display that by its very structure promotes looks to&#xD;
      distractors seems to be a potentially critical mistake.&#xD;
&#xD;
      The current study examines the effects of adding a communication partner on the allocation of&#xD;
      visual attention to optimally and non-optimally designed displays. This study is a&#xD;
      translational step of moving from more basic research towards more clinically relevant&#xD;
      research. Of interest are two questions: (1) How is a social partner integrated into the&#xD;
      attentional field of the individual using AAC, in general, and (2) What is the effect of the&#xD;
      introduction of the partner/social communication task on the patterns of attention across&#xD;
      different display conditions?&#xD;
&#xD;
      Participants who have participated in the earlier research of the PI will be contacted to see&#xD;
      if they would like to return for this one. Participants who express interest in learning more&#xD;
      will be sent the phone/email information, the recruitment flyer and, if they request it, the&#xD;
      consent form. If after reading these the participants are still interested, scheduling will&#xD;
      begin.&#xD;
&#xD;
      First, during the assessment, participants will be assessed with the Peabody Picture&#xD;
      Vocabulary Test - Fourth Edition (PPVT-4), which is an assessment of receptive vocabulary&#xD;
      skills. It is the gold standard in both speech and language assessments as well as research,&#xD;
      for estimating vocabulary size. In this, the child is shown four pictures at a time, and&#xD;
      asked to choose one of them on the basis of the spoken word. The test continues until the&#xD;
      child makes more than 8 errors in a set of 12. The test takes generally about 20 minutes to&#xD;
      complete.&#xD;
&#xD;
      After the assessment portion is completed, participants will return for up to five additional&#xD;
      sessions to undergo the storybook reading portion of the study. Each visit will involve&#xD;
      reading two separate books with a trained research assistant and should last about 30&#xD;
      minutes. Before reading the books, the research assistant will conduct a preference&#xD;
      assessment during which the participant will be given a choice of 4-6 possible sets of books&#xD;
      that he/she will read over the length of the study. The participant will be provided with&#xD;
      pictures of choices and can indicate their preferred choice by speaking, pointing to, or&#xD;
      selecting their choice.&#xD;
&#xD;
      While participants are engaged in book-reading, they will also be wearing Tobii Pro eye&#xD;
      tracking glasses that have an eye tracking device embedded within. It is ultra-lightweight&#xD;
      glasses with a highly unobtrusive head unit. Mobile eye tracking goggles allow for recording&#xD;
      of gaze path directly within the frames of glasses (similar to Google glasses). These mobile&#xD;
      technologies enable visualization and analysis of allocation of visual attention during live&#xD;
      social interactions, as the recording apparatus moves simultaneously with the movement of the&#xD;
      participant's head and records the changing field of vision. The technology uses extremely&#xD;
      low-level infrared light that is bounced off the pupil of the participant. The amount of&#xD;
      infrared light is smaller than that found in the typical television remote and is far below&#xD;
      federal safety requirements. The glasses include a non-invasive strap that will be tightened&#xD;
      at the back to ensure the glasses stay in place when they are worn.&#xD;
&#xD;
      Prior to placing the glasses on the participant, the research assistant will follow a&#xD;
      protocol to allow the participant to become familiar with the eye-tracking glasses. This will&#xD;
      involve watching a short video that shows another person wearing the glasses. Then, the&#xD;
      participant will be invited to place a pair of sunglasses on that has a strap similar to the&#xD;
      eye-tracking goggles. The research assistant will tighten the strap and allow the child to&#xD;
      wear the sunglasses for several minutes to become accustomed to the strap. Next, the&#xD;
      participant will be fitted with the eye-tracking glasses. If the child wears prescription&#xD;
      eye-glasses, the lenses in the glasses will be changed to match their prescription (there are&#xD;
      multiple lenses that can be switched in or out of the glasses themselves), which will be&#xD;
      obtained from the parents on the demographic form.&#xD;
&#xD;
      One the child is fitted with the eye-tracking glasses, the book reading will begin. The&#xD;
      research assistant will read a book to the participant. The participant will be positioned in&#xD;
      front of an AAC display that include symbols/messages to comment about the book. Each&#xD;
      participant will undergo several sessions of book reading. The participant will interact with&#xD;
      the partner during the book reading exchange by using an AAC display that contains symbols to&#xD;
      comment about the book. The participant may use the AAC display by accessing it with a mouse&#xD;
      or directly touching the symbols. The research assistant will follow a script that includes&#xD;
      different types of questions directed towards the participant.&#xD;
&#xD;
      In between the two books read each session, the participant will be offered a snack, that is&#xD;
      a snack approved by the parents/guardians prior to participation.&#xD;
&#xD;
      The sessions will be video recorded using a video camera. This will allow for post hoc review&#xD;
      to ensure the fidelity of adherence to the script by the trained research assistant.&#xD;
    </textblock>
  </detailed_description>
  <overall_status>Recruiting</overall_status>
  <start_date type="Actual">April 1, 2018</start_date>
  <completion_date type="Anticipated">November 30, 2022</completion_date>
  <primary_completion_date type="Anticipated">November 30, 2022</primary_completion_date>
  <phase>N/A</phase>
  <study_type>Interventional</study_type>
  <has_expanded_access>No</has_expanded_access>
  <study_design_info>
    <allocation>N/A</allocation>
    <intervention_model>Single Group Assignment</intervention_model>
    <intervention_model_description>Within subjects alternating treatment design</intervention_model_description>
    <primary_purpose>Device Feasibility</primary_purpose>
    <masking>None (Open Label)</masking>
  </study_design_info>
  <primary_outcome>
    <measure>Percent of visual fixation time on meaningful and non meaningful stimuli</measure>
    <time_frame>1-6 hours</time_frame>
    <description>Measured through percent of fixation time allocated to (a) the AAC display; (b) the storybook, or (c) the communication partner. Percent is calculated by dividing the total number of samples within each area (AAC display, storybook, partner) into the total number of samples obtained by the eye tracking device.</description>
  </primary_outcome>
  <primary_outcome>
    <measure>Number of times the participant communicates during the intervention</measure>
    <time_frame>1-6 hours</time_frame>
    <description>Rate of communication attempts during the storybook reading is defined as the number of times the participant attempts to communicate, divided by the total session duration.</description>
  </primary_outcome>
  <number_of_arms>1</number_of_arms>
  <enrollment type="Anticipated">15</enrollment>
  <condition>Down Syndrome</condition>
  <condition>Augmentative and Alternative Communication</condition>
  <arm_group>
    <arm_group_label>AAC Intervention</arm_group_label>
    <arm_group_type>Experimental</arm_group_type>
    <description>Participants will use AAC technology of different designs delivered on iMacs or Surface tablets</description>
  </arm_group>
  <intervention>
    <intervention_type>Device</intervention_type>
    <intervention_name>AAC Technology - Standard of Care</intervention_name>
    <description>Story Book is separate from AAC symbols, AAC symbols are arranged on a grid with color backgrounds. This is non-optimal arrangement and non-integrated presentation</description>
    <arm_group_label>AAC Intervention</arm_group_label>
    <other_name>Standard of Care</other_name>
  </intervention>
  <intervention>
    <intervention_type>Device</intervention_type>
    <intervention_name>AAC Technology - non-optimal integrated arrangement</intervention_name>
    <description>Story Book is integrated on to the AAC display together with the AAC symbols, AAC symbols are arranged on a grid with color backgrounds. This is non-optimal arrangement and but integrated presentation</description>
    <arm_group_label>AAC Intervention</arm_group_label>
    <other_name>Non-optimal, integrated</other_name>
  </intervention>
  <intervention>
    <intervention_type>Device</intervention_type>
    <intervention_name>AAC Technology - non-optimal integrated arrangement</intervention_name>
    <description>Story Book is integrated on to the AAC display together with the AAC symbols, AAC symbols are arranged on a grid with color backgrounds. This is optimal arrangement and but integrated presentation</description>
    <arm_group_label>AAC Intervention</arm_group_label>
    <other_name>Optimal, integrated</other_name>
  </intervention>
  <eligibility>
    <criteria>
      <textblock>
        Inclusion Criteria:&#xD;
&#xD;
          -  Participants with Down syndrome who have receptive language age estimates between 3;0&#xD;
             -7;0 years as measures on the Peabody Picture Vocabulary Test- 4th Edition (PPVT-IV;&#xD;
             Dunn &amp; Dunn, 2006) and chronological ages of 7 to 35 years.&#xD;
&#xD;
        Exclusion Criteria:&#xD;
&#xD;
          -  We will exclude anyone outside the range of 7-35 years, inclusive. We will plan to&#xD;
             exclude those having: (1) uncontrolled seizures; (2) sensory or peripheral impairment&#xD;
             that might impair performance; (3) co-morbid illnesses with implications for central&#xD;
             nervous system function.&#xD;
      </textblock>
    </criteria>
    <gender>All</gender>
    <minimum_age>7 Years</minimum_age>
    <maximum_age>35 Years</maximum_age>
    <healthy_volunteers>No</healthy_volunteers>
  </eligibility>
  <overall_official>
    <last_name>Krista Wilkinson, PhD</last_name>
    <role>Principal Investigator</role>
    <affiliation>Penn State</affiliation>
  </overall_official>
  <overall_contact>
    <last_name>Krista M Wilkinson, PhD</last_name>
    <phone>814-863-2206</phone>
    <email>kmw22@psu.edu</email>
  </overall_contact>
  <location>
    <facility>
      <name>11 Ford Building</name>
      <address>
        <city>University Park</city>
        <state>Pennsylvania</state>
        <zip>16802</zip>
        <country>United States</country>
      </address>
    </facility>
    <status>Recruiting</status>
    <contact>
      <last_name>Krista M Wilkinson, PhD</last_name>
      <phone>814-863-2206</phone>
      <email>kmw22@psu.edu</email>
    </contact>
  </location>
  <location_countries>
    <country>United States</country>
  </location_countries>
  <verification_date>March 2021</verification_date>
  <study_first_submitted>March 16, 2018</study_first_submitted>
  <study_first_submitted_qc>December 3, 2018</study_first_submitted_qc>
  <study_first_posted type="Actual">December 5, 2018</study_first_posted>
  <last_update_submitted>March 10, 2021</last_update_submitted>
  <last_update_submitted_qc>March 10, 2021</last_update_submitted_qc>
  <last_update_posted type="Actual">March 12, 2021</last_update_posted>
  <responsible_party>
    <responsible_party_type>Principal Investigator</responsible_party_type>
    <investigator_affiliation>Penn State University</investigator_affiliation>
    <investigator_full_name>Krista Wilkinson</investigator_full_name>
    <investigator_title>Professor</investigator_title>
  </responsible_party>
  <condition_browse>
    <!-- CAUTION:  The following MeSH terms are assigned with an imperfect algorithm            -->
    <mesh_term>Down Syndrome</mesh_term>
    <mesh_term>Syndrome</mesh_term>
  </condition_browse>
  <patient_data>
    <sharing_ipd>No</sharing_ipd>
  </patient_data>
  <!-- Results have not yet been posted for this study                                          -->
</clinical_study>


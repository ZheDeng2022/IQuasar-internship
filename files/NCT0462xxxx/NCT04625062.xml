<clinical_study>
  <!-- This xml conforms to an XML Schema at:
    https://clinicaltrials.gov/ct2/html/images/info/public.xsd -->
  <required_header>
    <download_date>ClinicalTrials.gov processed this data on November 19, 2021</download_date>
    <link_text>Link to the current ClinicalTrials.gov record.</link_text>
    <url>https://clinicaltrials.gov/show/NCT04625062</url>
  </required_header>
  <id_info>
    <org_study_id>C-RESULTS-TPT</org_study_id>
    <nct_id>NCT04625062</nct_id>
  </id_info>
  <brief_title>Comparing Traditional and Biofeedback Telepractice Treatment for Residual Speech Errors</brief_title>
  <acronym>C-RESULTS TPT</acronym>
  <official_title>Comparing Traditional and Biofeedback Treatment for Residual Speech Errors Via Telepractice</official_title>
  <sponsors>
    <lead_sponsor>
      <agency>New York University</agency>
      <agency_class>Other</agency_class>
    </lead_sponsor>
    <collaborator>
      <agency>Syracuse University</agency>
      <agency_class>Other</agency_class>
    </collaborator>
    <collaborator>
      <agency>Montclair State University</agency>
      <agency_class>Other</agency_class>
    </collaborator>
    <collaborator>
      <agency>National Institutes of Health (NIH)</agency>
      <agency_class>NIH</agency_class>
    </collaborator>
  </sponsors>
  <source>New York University</source>
  <oversight_info>
    <has_dmc>Yes</has_dmc>
    <is_fda_regulated_drug>No</is_fda_regulated_drug>
    <is_fda_regulated_device>No</is_fda_regulated_device>
  </oversight_info>
  <brief_summary>
    <textblock>
      This study aims to evaluate the relative efficacy of biofeedback and traditional treatment&#xD;
      for residual speech errors when both are delivered via telepractice. In a single-case&#xD;
      randomization design, up to eight children with RSE will receive both visual-acoustic&#xD;
      biofeedback and traditional treatment via telepractice. Acoustic measures of within-session&#xD;
      change will be compared across sessions randomly assigned to each condition. It is&#xD;
      hypothesized that participants will exhibit a clinically significant overall treatment&#xD;
      response and that short-term measures of change will indicate that biofeedback is associated&#xD;
      with larger increments of progress than traditional treatment.&#xD;
    </textblock>
  </brief_summary>
  <detailed_description>
    <textblock>
      The COVID-19 crisis has forced speech-language pathologists to migrate from in-person&#xD;
      delivery of speech treatment services to remote delivery via telepractice. This study will&#xD;
      compare the efficacy of visual-acoustic biofeedback treatment versus non-biofeedback&#xD;
      treatment in this setting. Specifically, participants will receive both visual-acoustic&#xD;
      biofeedback treatment and non-biofeedback treatment via telepractice (Zoom call with&#xD;
      screen-sharing) in a single-case randomization design. The hypothesis of interest is that&#xD;
      sessions featuring visual-acoustic biofeedback will be associated with larger short-term&#xD;
      gains than sessions featuring non-biofeedback treatment. To test this hypothesis, the study&#xD;
      team will recruit up to 8 participants who will receive an initial treatment orientation&#xD;
      followed by an equal dose of both types of treatment (10 sessions of visual-acoustic&#xD;
      biofeedback and 10 sessions of non-biofeedback treatment). Participants will complete&#xD;
      approximately two sessions per week via telepractice; each week will feature one session of&#xD;
      each type, randomly ordered. They will also complete 4 pre-treatment baseline sessions and 3&#xD;
      post-treatment maintenance sessions to evaluate the overall magnitude of change over the&#xD;
      course of treatment.&#xD;
    </textblock>
  </detailed_description>
  <overall_status>Completed</overall_status>
  <start_date type="Actual">September 1, 2020</start_date>
  <completion_date type="Actual">March 18, 2021</completion_date>
  <primary_completion_date type="Actual">March 18, 2021</primary_completion_date>
  <phase>Phase 1</phase>
  <study_type>Interventional</study_type>
  <has_expanded_access>No</has_expanded_access>
  <study_design_info>
    <allocation>Randomized</allocation>
    <intervention_model>Factorial Assignment</intervention_model>
    <intervention_model_description>In this single-case randomization design, each participant will receive an equal number of sessions of visual-acoustic biofeedback and traditional treatment (n = 10 each), with randomized allocation of treatment types to individual sessions. Randomization will be blocked, with each week of treatment serving as a block; within each week/block, one session will be randomly assigned to feature visual-acoustic and one to feature traditional treatment.</intervention_model_description>
    <primary_purpose>Treatment</primary_purpose>
    <masking>Single (Outcomes Assessor)</masking>
    <masking_description>All perceptual ratings will be obtained from blinded, naive listeners recruited through online crowdsourcing. Following protocols refined in previous published research, binary rating responses will be aggregated over at least 9 unique listeners per token. Acoustic measures will be obtained by trained research assistants who are blinded to the time point and treatment condition of each sample.</masking_description>
  </study_design_info>
  <primary_outcome>
    <measure>Acoustically measured accuracy of /r/ in words</measure>
    <time_frame>Immediately before each of 20 treatment sessions over 10 weeks</time_frame>
    <description>Probes of words and syllables containing /r/ in various phonetic contexts will be elicited at the start and end of treatment. We follow a semi-automated protocol to demarcate intervals of interest in a Praat textgrid. Textgrids and recordings are submitted to a forced aligner, followed by automated extraction of formant frequencies (F1, F2, F3) at the center of each /r/ interval identified. F3-F2 distance, which is smaller in correct /r/, serves as the primary acoustic correlate of accuracy.</description>
  </primary_outcome>
  <primary_outcome>
    <measure>Acoustically measured accuracy of /r/ in words</measure>
    <time_frame>Immediately after each of 20 treatment sessions over 10 weeks</time_frame>
    <description>Probes of words and syllables containing /r/ in various phonetic contexts will be elicited at the start and end of treatment. We follow a semi-automated protocol to demarcate intervals of interest in a Praat textgrid. Textgrids and recordings are submitted to a forced aligner, followed by automated extraction of formant frequencies (F1, F2, F3) at the center of each /r/ interval identified. F3-F2 distance, which is smaller in correct /r/, serves as the primary acoustic correlate of accuracy.</description>
  </primary_outcome>
  <primary_outcome>
    <measure>Acoustically measured accuracy of /r/ in words</measure>
    <time_frame>In three separate sessions within two weeks of the start of all treatment</time_frame>
    <description>Probes of words and syllables containing /r/ in various phonetic contexts will be elicited at the start and end of treatment. We follow a semi-automated protocol to demarcate intervals of interest in a Praat textgrid. Textgrids and recordings are submitted to a forced aligner, followed by automated extraction of formant frequencies (F1, F2, F3) at the center of each /r/ interval identified. F3-F2 distance, which is smaller in correct /r/, serves as the primary acoustic correlate of accuracy.</description>
  </primary_outcome>
  <primary_outcome>
    <measure>Acoustically measured accuracy of /r/ in words</measure>
    <time_frame>In three separate sessions within two weeks after the end of all treatment</time_frame>
    <description>Probes of words and syllables containing /r/ in various phonetic contexts were elicited at the start and end of treatment. We follow a semi-automated protocol to demarcate intervals of interest in a Praat textgrid. Textgrids and recordings are submitted to a forced aligner, followed by automated extraction of formant frequencies (F1, F2, F3) at the center of each /r/ interval identified. F3-F2 distance, which is smaller in correct /r/, serves as the primary acoustic correlate of accuracy.</description>
  </primary_outcome>
  <secondary_outcome>
    <measure>Proportion of &quot;correct&quot; (vs &quot;incorrect&quot;) ratings by blinded naive listeners, a measure of perceptually rated accuracy of /r/ production, for /r/ sounds produced in word or syllable probes.</measure>
    <time_frame>In three separate sessions within two weeks of the start of all treatment</time_frame>
    <description>To assess generalization of treatment gains to untreated words, participants will read probe lists eliciting /r/ in various phonetic contexts. Stimuli in each probe will be presented individually in randomized order. No auditory models will be provided; for children with reading difficulty, semantic cues will be provided to elicit the intended word. Individual words will be isolated from the audio record of each word probe and presented in randomized order for binary rating (correct/incorrect) by naive listeners who are blind to treatment condition and time point (but will see the written representation of each target word). We will use the proportion of &quot;correct&quot; ratings for each token as our primary measure of perceptually rated accuracy.</description>
  </secondary_outcome>
  <secondary_outcome>
    <measure>Proportion of &quot;correct&quot; (vs &quot;incorrect&quot;) ratings by blinded naive listeners, a measure of perceptually rated accuracy of /r/ production, for /r/ sounds produced in word or syllable probes.</measure>
    <time_frame>In three separate sessions within two weeks after the end of all treatment</time_frame>
    <description>To assess generalization of treatment gains to untreated words, participants will read probe lists eliciting /r/ in various phonetic contexts. Stimuli in each probe will be presented individually in randomized order. No auditory models will be provided; for children with reading difficulty, semantic cues will be provided to elicit the intended word. Individual words will be isolated from the audio record of each word probe and presented in randomized order for binary rating (correct/incorrect) by naive listeners who are blind to treatment condition and time point (but will see the written representation of each target word). We will use the proportion of &quot;correct&quot; ratings for each token as our primary measure of perceptually rated accuracy.</description>
  </secondary_outcome>
  <number_of_arms>2</number_of_arms>
  <enrollment type="Actual">7</enrollment>
  <condition>Speech Sound Disorder</condition>
  <arm_group>
    <arm_group_label>Condition 1: Visual-acoustic biofeedback</arm_group_label>
    <arm_group_type>Experimental</arm_group_type>
    <description>Visual-acoustic biofeedback treatment (behavioral) administered via telepractice</description>
  </arm_group>
  <arm_group>
    <arm_group_label>Condition 2: Motor-based treatment</arm_group_label>
    <arm_group_type>Experimental</arm_group_type>
    <description>Motor-based articulation treatment administered via telepractice</description>
  </arm_group>
  <intervention>
    <intervention_type>Behavioral</intervention_type>
    <intervention_name>Visual-acoustic biofeedback</intervention_name>
    <description>In visual-acoustic biofeedback treatment, the elements of motor-based treatment (i.e., auditory models and verbal descriptions of articulator placement) are enhanced with a dynamic display of the speech signal in the form of the real-time LPC (Linear Predictive Coding) spectrum. Because correct vs incorrect productions of /r/ contrast acoustically in the frequency of the third formant (F3), participants will be cued to make their real-time LPC spectrum match a visual target characterized by a low F3 frequency. They will be encouraged to attend to the visual display while adjusting the placement of their articulators and observing how those adjustments impact F3. All treatment will be provided over video calls.</description>
    <arm_group_label>Condition 1: Visual-acoustic biofeedback</arm_group_label>
  </intervention>
  <intervention>
    <intervention_type>Behavioral</intervention_type>
    <intervention_name>Motor-based treatment</intervention_name>
    <description>Motor-based articulation treatment involves providing auditory models and verbal descriptions of correct articulator placement, then cueing repetitive motor practice. Images and diagrams of the vocal tract will be used as visual aids; however, no real-time visual display of articulatory or acoustic information will be made available. All treatment will be provided over video calls.</description>
    <arm_group_label>Condition 2: Motor-based treatment</arm_group_label>
    <other_name>Traditional treatment</other_name>
  </intervention>
  <eligibility>
    <criteria>
      <textblock>
        Inclusion Criteria:&#xD;
&#xD;
          -  Must be between 9;0 and 15;11 years of age at the time of enrollment.&#xD;
&#xD;
          -  Must speak English as the dominant language (i.e., must have begun learning English by&#xD;
             age 2, per parent report).&#xD;
&#xD;
          -  Must speak a rhotic dialect of English.&#xD;
&#xD;
          -  Must pass a brief examination of oral structure and function.&#xD;
&#xD;
          -  Must exhibit less than thirty percent accuracy, based on trained listener ratings, on&#xD;
             a probe list eliciting /r/ in various phonetic contexts at the word level.&#xD;
&#xD;
        Exclusion Criteria:&#xD;
&#xD;
          -  Must not receive a T score more than 1.3 standard deviations (SD) below the mean on&#xD;
             the Wechsler Abbreviated Scale of Intelligence-2 (WASI-2) Matrix Reasoning.&#xD;
&#xD;
          -  Must not receive a scaled score below 6 on the CELF-5 Recalling Sentences or&#xD;
             Formulated Sentences subtests.&#xD;
&#xD;
          -  Must not have history of sensorineural hearing loss or failed infant hearing&#xD;
             screening.&#xD;
&#xD;
          -  Must not have an existing diagnosis of developmental disability, major neurobehavioral&#xD;
             syndrome such as cerebral palsy, Down Syndrome, or Autism Spectrum Disorder, or major&#xD;
             neural disorder (e.g., epilepsy, agenesis of the corpus callosum) or insult (e.g.,&#xD;
             traumatic brain injury, stroke, or tumor resection).&#xD;
&#xD;
          -  Must not show clinically significant signs of apraxia of speech or dysarthria.&#xD;
&#xD;
          -  Must not have major orthodontia that could interfere with tongue-palate contact.&#xD;
      </textblock>
    </criteria>
    <gender>All</gender>
    <minimum_age>9 Years</minimum_age>
    <maximum_age>15 Years</maximum_age>
    <healthy_volunteers>No</healthy_volunteers>
  </eligibility>
  <location>
    <facility>
      <name>Montclair State University</name>
      <address>
        <city>Bloomfield</city>
        <state>New Jersey</state>
        <zip>07003</zip>
        <country>United States</country>
      </address>
    </facility>
  </location>
  <location>
    <facility>
      <name>Syracuse University</name>
      <address>
        <city>Syracuse</city>
        <state>New York</state>
        <zip>13244</zip>
        <country>United States</country>
      </address>
    </facility>
  </location>
  <location_countries>
    <country>United States</country>
  </location_countries>
  <reference>
    <citation>Dugan SH, Silbert N, McAllister T, Preston JL, Sotto C, Boyce SE. Modelling category goodness judgments in children with residual sound errors. Clin Linguist Phon. 2019;33(4):295-315. doi: 10.1080/02699206.2018.1477834. Epub 2018 May 24.</citation>
    <PMID>29792525</PMID>
  </reference>
  <reference>
    <citation>Campbell H, Harel D, Hitchcock E, McAllister Byun T. Selecting an acoustic correlate for automated measurement of American English rhotic production in children. Int J Speech Lang Pathol. 2018 Nov;20(6):635-643. doi: 10.1080/17549507.2017.1359334. Epub 2017 Aug 10.</citation>
    <PMID>28795872</PMID>
  </reference>
  <reference>
    <citation>Campbell H, McAllister Byun T. Deriving individualised /r/ targets from the acoustics of children's non-rhotic vowels. Clin Linguist Phon. 2018;32(1):70-87. doi: 10.1080/02699206.2017.1330898. Epub 2017 Jul 13.</citation>
    <PMID>28703653</PMID>
  </reference>
  <reference>
    <citation>McAllister Byun T. Efficacy of Visual-Acoustic Biofeedback Intervention for Residual Rhotic Errors: A Single-Subject Randomization Study. J Speech Lang Hear Res. 2017 May 24;60(5):1175-1193. doi: 10.1044/2016_JSLHR-S-16-0038.</citation>
    <PMID>28389677</PMID>
  </reference>
  <reference>
    <citation>McAllister Byun T, Tiede M. Perception-production relations in later development of American English rhotics. PLoS One. 2017 Feb 16;12(2):e0172022. doi: 10.1371/journal.pone.0172022. eCollection 2017.</citation>
    <PMID>28207800</PMID>
  </reference>
  <reference>
    <citation>McAllister Byun T, Campbell H. Differential Effects of Visual-Acoustic Biofeedback Intervention for Residual Speech Errors. Front Hum Neurosci. 2016 Nov 11;10:567. eCollection 2016.</citation>
    <PMID>27891084</PMID>
  </reference>
  <reference>
    <citation>McAllister Byun T, Halpin PF, Szeredi D. Online crowdsourcing for efficient rating of speech: a validation study. J Commun Disord. 2015 Jan-Feb;53:70-83. doi: 10.1016/j.jcomdis.2014.11.003. Epub 2014 Dec 15.</citation>
    <PMID>25578293</PMID>
  </reference>
  <reference>
    <citation>Harel D, Hitchcock ER, Szeredi D, Ortiz J, McAllister Byun T. Finding the experts in the crowd: Validity and reliability of crowdsourced measures of children's gradient speech contrasts. Clin Linguist Phon. 2017;31(1):104-117. Epub 2016 Jun 7.</citation>
    <PMID>27267258</PMID>
  </reference>
  <reference>
    <citation>Hitchcock ER, Harel D, Byun TM. Social, Emotional, and Academic Impact of Residual Speech Errors in School-Aged Children: A Survey Study. Semin Speech Lang. 2015 Nov;36(4):283-94. doi: 10.1055/s-0035-1562911. Epub 2015 Oct 12.</citation>
    <PMID>26458203</PMID>
  </reference>
  <reference>
    <citation>Byun TM, Hitchcock ER. Investigating the use of traditional and spectral biofeedback approaches to intervention for /r/ misarticulation. Am J Speech Lang Pathol. 2012 Aug;21(3):207-21. doi: 10.1044/1058-0360(2012/11-0083). Epub 2012 Mar 21.</citation>
    <PMID>22442281</PMID>
  </reference>
  <verification_date>November 2020</verification_date>
  <study_first_submitted>September 8, 2020</study_first_submitted>
  <study_first_submitted_qc>November 5, 2020</study_first_submitted_qc>
  <study_first_posted type="Actual">November 12, 2020</study_first_posted>
  <last_update_submitted>October 12, 2021</last_update_submitted>
  <last_update_submitted_qc>October 12, 2021</last_update_submitted_qc>
  <last_update_posted type="Actual">October 13, 2021</last_update_posted>
  <responsible_party>
    <responsible_party_type>Sponsor</responsible_party_type>
  </responsible_party>
  <keyword>speech</keyword>
  <keyword>articulation</keyword>
  <keyword>motor development</keyword>
  <condition_browse>
    <!-- CAUTION:  The following MeSH terms are assigned with an imperfect algorithm            -->
    <mesh_term>Speech Sound Disorder</mesh_term>
  </condition_browse>
  <patient_data>
    <sharing_ipd>No</sharing_ipd>
  </patient_data>
  <!-- Results have not yet been posted for this study                                          -->
</clinical_study>


<clinical_study>
  <!-- This xml conforms to an XML Schema at:
    https://clinicaltrials.gov/ct2/html/images/info/public.xsd -->
  <required_header>
    <download_date>ClinicalTrials.gov processed this data on November 19, 2021</download_date>
    <link_text>Link to the current ClinicalTrials.gov record.</link_text>
    <url>https://clinicaltrials.gov/show/NCT03174314</url>
  </required_header>
  <id_info>
    <org_study_id>17-00317</org_study_id>
    <nct_id>NCT03174314</nct_id>
  </id_info>
  <brief_title>Feasibility and Efficacy of Assisstive Tactile and Auditory Communicating Devices</brief_title>
  <acronym>VIS4ION</acronym>
  <official_title>Feasibility and Efficacy of the VIS4ION Platform and Assistive Tactile and Auditory Communicating Devices in Low Vision Subjects</official_title>
  <sponsors>
    <lead_sponsor>
      <agency>NYU Langone Health</agency>
      <agency_class>Other</agency_class>
    </lead_sponsor>
  </sponsors>
  <source>NYU Langone Health</source>
  <oversight_info>
    <has_dmc>No</has_dmc>
    <is_fda_regulated_drug>No</is_fda_regulated_drug>
    <is_fda_regulated_device>No</is_fda_regulated_device>
  </oversight_info>
  <brief_summary>
    <textblock>
      This pilot study will integrate multi-sensor fusion techniques (software) to effectively&#xD;
      combine information obtained from the newly embedded infrared, ultrasound, and&#xD;
      stereo-camera-based sensor systems (hardware) that are implemented into the VIS4ION platform.&#xD;
&#xD;
      The core of this technology is based on 4 components: (1) a wearable vest with several&#xD;
      distinct range and image sensors embedded. These sensors extract pertinent information about&#xD;
      obstacles and the environment, which are conveyed to (2) a haptic interface (belt) that&#xD;
      communicates this spatial information to the end-user in real-time via an intuitive,&#xD;
      ergonomic and personalized vibrotactile re-display along the torso. (3) A smartphone serves&#xD;
      as a connectivity gateway and coordinates the core components through WiFi, bluetooth, and/or&#xD;
      4G LTE, (4) a headset that contains both binaural, open-ear, bone conduction speakers&#xD;
      (leaving the ear canal patent for ambient sounds) and a microphone for oral&#xD;
      communication-based voice recognition during use of a virtual personal assistant (VPA).&#xD;
&#xD;
      Blindfolded-sighted (50), and blind (50) subjects in a real-world, combined obstacle&#xD;
      avoidance / navigation task will serve as an independent measure of overall improvements in&#xD;
      the system as well as a roadmap for future avenues to enhance performance.&#xD;
    </textblock>
  </brief_summary>
  <detailed_description>
    <textblock>
      Investigators will try to develop algorithms that will recognize multiple objects and persons&#xD;
      in real-time (enhanced scene interpretation for multi-object identification). And based on&#xD;
      that, human-centered simulation trials and experiments for feasibility and efficacy of the&#xD;
      platform's tactile and auditory 'communication' outputs will be conducted. Finally auditory&#xD;
      and tactile 'prompts' (system output) based on the end user's immediate needs based on&#xD;
      initial testing results, will be integrated into the platform.&#xD;
    </textblock>
  </detailed_description>
  <overall_status>Active, not recruiting</overall_status>
  <start_date type="Actual">July 26, 2017</start_date>
  <completion_date type="Anticipated">December 1, 2023</completion_date>
  <primary_completion_date type="Anticipated">December 1, 2023</primary_completion_date>
  <phase>N/A</phase>
  <study_type>Interventional</study_type>
  <has_expanded_access>No</has_expanded_access>
  <study_design_info>
    <allocation>Non-Randomized</allocation>
    <intervention_model>Parallel Assignment</intervention_model>
    <primary_purpose>Treatment</primary_purpose>
    <masking>None (Open Label)</masking>
  </study_design_info>
  <primary_outcome>
    <measure>Real-time situational obstacle awareness measured by detection threshold</measure>
    <time_frame>6 Months</time_frame>
    <description>Ability of observers to, in real-time, interpret and respond to platform outputs will be tested</description>
  </primary_outcome>
  <primary_outcome>
    <measure>Real-time situational obstacle awareness measured by change in time</measure>
    <time_frame>6 Months</time_frame>
    <description>Ability of observers to, in real-time, interpret and respond to platform outputs will be tested; ∆t averages (separately by in-/correct)</description>
  </primary_outcome>
  <number_of_arms>2</number_of_arms>
  <enrollment type="Actual">100</enrollment>
  <condition>Visual Impairment</condition>
  <arm_group>
    <arm_group_label>50 visually impaired</arm_group_label>
    <arm_group_type>Experimental</arm_group_type>
    <description>50 visually impaired subjects will provide continuity across iterations of the testing, allowing for direct comparisons of responses within an individual for a single behavioral measure across different iterations of the device architecture and output configuration.</description>
  </arm_group>
  <arm_group>
    <arm_group_label>50 healthy controls</arm_group_label>
    <arm_group_type>Active Comparator</arm_group_type>
    <description>50 healthy (naive) subjects invaluable insights as well as a range of body types, cognitive abilities, and other idiosyncrasies that will keep our design and testing process from tailoring the device to a small set of individuals rather than the broader population.</description>
  </arm_group>
  <intervention>
    <intervention_type>Behavioral</intervention_type>
    <intervention_name>standard object battery and training sequence</intervention_name>
    <description>Used to examine human localization and identification performance. Object battery will include: trip hazards ('pucks' with various height/width combinations to represent children's toys, street debris, rocks, pets, etc.), furniture (chairs, desks, couches, benches), people, walls, and corridors. Objects will be recorded by placing them to the right or left on the path-of-travel. Raw sensor outputs for single obstacles and walls in isolation (trip-hazard pucks with 8 heights, chair, desk, person, wall) and the same set of single obstacles against a wall. This set of 23 raw sensor traces can be used in various combinations (e.g., encountering a curb and then a person, or a chair) to create a training sequence that will help experimentally naïve subjects to understand the correspondence between tactile stimulation and the real-world scenario it is intended to depict.</description>
    <arm_group_label>50 healthy controls</arm_group_label>
    <arm_group_label>50 visually impaired</arm_group_label>
  </intervention>
  <eligibility>
    <criteria>
      <textblock>
        Inclusion Criteria:&#xD;
&#xD;
          -  People with visual impairments of all different levels and etiologies.&#xD;
&#xD;
        Exclusion Criteria:&#xD;
&#xD;
          -  Significant cognitive dysfunction (score &lt;24 on Folsteins' Mini Mental Status&#xD;
             Examination)&#xD;
&#xD;
          -  Previous neurological illness, complicated medical condition;&#xD;
&#xD;
          -  Significant mobility restrictions; people using walkers and wheelchairs&#xD;
&#xD;
          -  Pregnancy&#xD;
      </textblock>
    </criteria>
    <gender>All</gender>
    <minimum_age>18 Years</minimum_age>
    <maximum_age>80 Years</maximum_age>
    <healthy_volunteers>Accepts Healthy Volunteers</healthy_volunteers>
  </eligibility>
  <overall_official>
    <last_name>John R Rizzo, MD</last_name>
    <role>Principal Investigator</role>
    <affiliation>NYU Langone Health</affiliation>
  </overall_official>
  <location>
    <facility>
      <name>New York University School of Medicine</name>
      <address>
        <city>New York</city>
        <state>New York</state>
        <zip>10016</zip>
        <country>United States</country>
      </address>
    </facility>
  </location>
  <location_countries>
    <country>United States</country>
  </location_countries>
  <verification_date>January 2021</verification_date>
  <study_first_submitted>May 30, 2017</study_first_submitted>
  <study_first_submitted_qc>May 30, 2017</study_first_submitted_qc>
  <study_first_posted type="Actual">June 2, 2017</study_first_posted>
  <last_update_submitted>January 4, 2021</last_update_submitted>
  <last_update_submitted_qc>January 4, 2021</last_update_submitted_qc>
  <last_update_posted type="Actual">January 5, 2021</last_update_posted>
  <responsible_party>
    <responsible_party_type>Sponsor</responsible_party_type>
  </responsible_party>
  <keyword>Visual Impairment</keyword>
  <condition_browse>
    <!-- CAUTION:  The following MeSH terms are assigned with an imperfect algorithm            -->
    <mesh_term>Vision Disorders</mesh_term>
    <mesh_term>Vision, Low</mesh_term>
  </condition_browse>
  <patient_data>
    <sharing_ipd>Undecided</sharing_ipd>
  </patient_data>
  <!-- Results have not yet been posted for this study                                          -->
</clinical_study>

